{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579d5656",
   "metadata": {},
   "source": [
    "## Simple Langchain Gen-AI App\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affbcbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf37caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 14:13:05.000316: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-07 14:13:05.045301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-07 14:13:06.215555: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/abhi/.virtualenvs/my_genai_venv/lib/python3.11/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x702d90a2f990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion from Website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/oss/python/langchain/knowledge-base\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39356c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/knowledge-base', 'title': 'Build a semantic search engine with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a semantic search engine with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a semantic search engine with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsSetupInstallationLangSmith1. Documents and Document LoadersLoading documentsSplitting2. Embeddings3. Vector stores4. RetrieversNext stepsTutorialsLangChainBuild a semantic search engine with LangChainCopy pageCopy page\\u200bOverview\\nThis tutorial will familiarize you with LangChain’s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data—  from (vector) databases and other sources — for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\\n\\u200bConcepts\\nThis guide focuses on retrieval of text data. We will cover the following concepts:\\n\\nDocuments and document loaders;\\nText splitters;\\nEmbeddings;\\nVector stores and retrievers.\\n\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires the langchain-community and pypdf packages:\\npipcondaCopypip install langchain-community pypdf\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, if in a notebook, you can set them with:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200b1. Documents and Document Loaders\\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\\n\\npage_content: a string representing the content;\\nmetadata: a dict containing arbitrary metadata;\\nid: (optional) a string identifier for the document.\\n\\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\\nWe can generate sample documents when desired:\\nCopyfrom langchain_core.documents import Document\\n\\ndocuments = [\\n    Document(\\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\\n        metadata={\"source\": \"mammal-pets-doc\"},\\n    ),\\n    Document(\\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\\n        metadata={\"source\": \"mammal-pets-doc\"},\\n    ),\\n]\\n\\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\\n\\u200bLoading documents\\nLet’s load a PDF into a sequence of Document objects. Here is a sample PDF — a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders.\\nCopyfrom langchain_community.document_loaders import PyPDFLoader\\n\\nfile_path = \"../example_data/nke-10k-2023.pdf\"\\nloader = PyPDFLoader(file_path)\\n\\ndocs = loader.load()\\n\\nprint(len(docs))\\n\\nCopy107\\n\\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\\n\\nThe string content of the page;\\nMetadata containing the file name and page number.\\n\\nCopyprint(f\"{docs[0].page_content[:200]}\\\\n\")\\nprint(docs[0].metadata)\\n\\nCopyTable of Contents\\nUNITED STATES\\nSECURITIES AND EXCHANGE COMMISSION\\nWashington, D.C. 20549\\nFORM 10-K\\n(Mark One)\\n☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFO\\n\\n{\\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'page\\': 0}\\n\\n\\u200bSplitting\\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not “washed out” by surrounding text.\\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index where each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute “start_index”.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(len(all_splits))\\n\\nCopy514\\n\\n\\u200b2. Embeddings\\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let’s select a model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nCopyvector_1 = embeddings.embed_query(all_splits[0].page_content)\\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\\n\\nassert len(vector_1) == len(vector_2)\\nprint(f\"Generated vectors of length {len(vector_1)}\\\\n\")\\nprint(vector_1[:10])\\n\\nCopyGenerated vectors of length 1536\\n\\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\\n\\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\\n\\u200b3. Vector stores\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let’s select a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\nHaving instantiated our vector store, we can now index the documents.\\nCopyids = vector_store.add_documents(documents=all_splits)\\n\\nNote that most vector store implementations will allow you to connect to an existing vector store—  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\\nOnce we’ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\\n\\nSynchronously and asynchronously;\\nBy string query and by vector;\\nWith and without returning similarity scores;\\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\\n\\nThe methods will generally include a list of Document objects in their outputs.\\nUsage\\nEmbeddings typically represent text as a “dense” vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\\nReturn documents based on similarity to a string query:\\nCopyresults = vector_store.similarity_search(\\n    \"How many distribution centers does Nike have in the US?\"\\n)\\n\\nprint(results[0])\\n\\nCopypage_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213\\nNIKE Brand in-line stores (including employee-only stores) 74\\nConverse stores (including factory stores) 82\\nTOTAL 369\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\n\\nAsync query:\\nCopyresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\\n\\nprint(results[0])\\n\\nCopypage_content=\\'Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\' metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nReturn scores:\\nCopy# Note that providers implement different scores; the score here\\n# is a distance metric that varies inversely with similarity.\\n\\nresults = vector_store.similarity_search_with_score(\"What was Nike\\'s revenue in 2023?\")\\ndoc, score = results[0]\\nprint(f\"Score: {score}\\\\n\")\\nprint(doc)\\n\\nCopyScore: 0.23699893057346344\\n\\npage_content=\\'Table of Contents\\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\\nFISCAL 2023 COMPARED TO FISCAL 2022\\n•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\\n•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\\nincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\\nequivalent basis.\\' metadata={\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nReturn documents based on similarity to an embedded query:\\nCopyembedding = embeddings.embed_query(\"How were Nike\\'s margins impacted in 2023?\")\\n\\nresults = vector_store.similarity_search_by_vector(embedding)\\nprint(results[0])\\n\\nCopypage_content=\\'Table of Contents\\nGROSS MARGIN\\nFISCAL 2023 COMPARED TO FISCAL 2022\\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\\n*Wholesale equivalent\\nThe decrease in gross margin for fiscal 2023 was primarily due to:\\n•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\\nproduct mix;\\n•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\\nthe prior period resulting from lower available inventory supply;\\n•Unfavorable changes in net foreign currency exchange rates, including hedges; and\\n•Lower off-price margin, on a wholesale equivalent basis.\\nThis was partially offset by:\\' metadata={\\'page\\': 36, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n\\nLearn more:\\n\\nAPI Reference\\nIntegration-specific docs\\n\\n\\u200b4. Retrievers\\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\\nCopyfrom typing import List\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.runnables import chain\\n\\n\\n@chain\\ndef retriever(query: str) -> List[Document]:\\n    return vector_store.similarity_search(query, k=1)\\n\\n\\nretriever.batch(\\n    [\\n        \"How many distribution centers does Nike have in the US?\",\\n        \"When was Nike incorporated?\",\\n    ],\\n)\\n\\nCopy[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')],\\n [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\n\\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\\nCopyretriever = vector_store.as_retriever(\\n    search_type=\"similarity\",\\n    search_kwargs={\"k\": 1},\\n)\\n\\nretriever.batch(\\n    [\\n        \"How many distribution centers does Nike have in the US?\",\\n        \"When was Nike incorporated?\",\\n    ],\\n)\\n\\nCopy[[Document(metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}, page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\\\nU.S. RETAIL STORES NUMBER\\\\nNIKE Brand factory stores 213 \\\\nNIKE Brand in-line stores (including employee-only stores) 74 \\\\nConverse stores (including factory stores) 82 \\\\nTOTAL 369 \\\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\\\n2023 FORM 10-K 2\\')],\\n [Document(metadata={\\'page\\': 3, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}, page_content=\\'Table of Contents\\\\nPART I\\\\nITEM 1. BUSINESS\\\\nGENERAL\\\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales\\')]]\\n\\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\\n\\u200bNext steps\\nYou’ve now seen how to build a semantic search engine over a PDF document.\\nFor more on document loaders:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on embeddings:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on vector stores:\\n\\nOverview\\nAvailable integrations\\n\\nFor more on RAG, see:\\n\\nBuild a Retrieval Augmented Generation (RAG) App\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoLearnPreviousBuild a RAG agent with LangChainNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide documents into chunks -> Convert to Embeddings -> Store in Vector DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splits = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce54f191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0602f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x702d8ab976d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x702d8a5e80d0>, model='text-embedding-3-large', dimensions=None, deployment=None, openai_api_version='2024-12-01-preview', openai_api_base=None, openai_api_type='azure', openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=2048, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True, azure_endpoint='https://pfs-2-namit-resource.cognitiveservices.azure.com/', azure_ad_token=None, azure_ad_token_provider=None, azure_ad_async_token_provider=None, validate_base_url=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "api_version = \"2024-12-01-preview\" \n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    model = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_MODEL\"),\n",
    "    api_version = api_version\n",
    ")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db35a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x702d8a5eb1d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_db = FAISS.from_documents(doc_splits, embeddings)\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8047f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d075df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_LLM_MODEL\"),\n",
    "    api_version = \"2025-01-01-preview\",\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671af509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1cafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08845180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To build a semantic search engine using **LangChain**, you typically follow these main steps (as outlined in the LangChain documentation):\\n\\n1. **Load Documents** – Start by gathering your data and loading it into LangChain using *Document Loaders*. These loaders handle different data sources such as local files, web pages, or APIs.\\n\\n2. **Split Documents** – Break large documents into smaller, more manageable text chunks. This helps improve retrieval accuracy since embeddings work best on moderate-length text segments.\\n\\n3. **Create Embeddings** – Convert text chunks into numerical vector representations using an *Embedding Model* (for example, OpenAI embeddings or other supported models).\\n\\n4. **Store in a Vector Store** – Save the embeddings and their associated text in a *Vector Store* (like FAISS, Chroma, or Pinecone). This enables efficient similarity search.\\n\\n5. **Set Up a Retriever** – Build a retriever interface that queries the vector store to find and return the most relevant chunks for a given user query based on semantic similarity.\\n\\n6. **Integrate a Language Model (Optional)** – For advanced retrieval-augmented generation (RAG), combine the retriever with an LLM to produce context-aware answers.\\n\\n7. **Evaluate and Iterate** – Use tools like *LangSmith* to monitor and improve your system’s performance.\\n\\nIn short:  \\n**Documents → Embeddings → Vector Store → Retriever → (Optional LLM for responses)**  \\n\\nThat pipeline forms the foundation of a semantic search engine built with LangChain.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\"input\":\"How to build a semantic search enginer using LangChain?\", \n",
    "\"context\": doc_splits[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596a29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Documents to come from retreiver\n",
    "retreiver = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529b73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc774d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retreiver | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab38f9c",
   "metadata": {},
   "source": [
    "This is the heart of our pipeline. Let’s unpack it.\n",
    "\n",
    "**Step 1 — Parallel Stage: `{\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}`**\n",
    "\n",
    "This is a dictionary of runnables. Because it’s a dict, this part of the pipeline becomes a parallel stage.\n",
    "\n",
    "What each key does\n",
    "\n",
    "* context:\n",
    "    * The retriever is invoked with the user’s question.\n",
    "    * It returns a list of docs.\n",
    "    * format_docs then runs your format_docs() function on the docs.\n",
    "    * The result is a string of text containing all relevant document content.\n",
    "\n",
    "* input:\n",
    "    * RunnablePassthrough() simply passes the user’s input through unchanged.\n",
    "    * This is needed because after parallel retrieval, you still need the original user query visible to the next stage.\n",
    "\n",
    "So after this step, the data shape is: `{\n",
    "    \"context\": \"some text from docs…\",\n",
    "    \"input\": \"What is X?\"\n",
    "}`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Step 2 — Prompt Template: `| prompt`**\n",
    "\n",
    "Here, prompt is a Runnable that expects a dict. The runnable takes the small dict from the previous step and formats it into a prompt string for the LLM.\n",
    "\n",
    "In current case we have 2 variables in PromptTemplate so it expects from previous stage `{\"context\": ..., \"input\": ...}`\n",
    "\n",
    "If our prompt had three placeholders, then it would expect: `{\"var1\": ..., \"var2\": ..., \"var3\": ...}`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Step 3 — LLM Call: `| llm`**\n",
    "\n",
    "The output of the prompt runnable (the filled-in prompt text) becomes the input to the AzureChatOpenAI LLM.\n",
    "\n",
    "This step calls the Azure endpoint with: prompt text, deployed model API key, etc. and produces raw LLM output.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Step 4 — Output Parsing: `| StrOutputParser()`**\n",
    "\n",
    "Finally, we chain a parser runnable that:\n",
    "* takes raw LLM output\n",
    "* returns a cleaned up string\n",
    "\n",
    "This replaces older output cleaning logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425802d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To build a semantic search engine using **LangChain**, follow these main steps:\\n\\n1. **Load and Prepare Documents**  \\n   - Use LangChain’s **document loaders** (for example, the `pypdf` loader) to import your source data, such as a PDF file.  \\n   - Split longer texts into smaller chunks using **text splitters** so that embeddings can be computed efficiently and with meaningful context.\\n\\n2. **Generate Text Embeddings**  \\n   - Choose an **embedding model** to convert text chunks into numerical vectors.  \\n   - These vectors capture semantic meaning, enabling similarity-based retrieval.\\n\\n3. **Store Embeddings in a Vector Store**  \\n   - Initialize a **VectorStore** (e.g., FAISS, Chroma, Pinecone, or another supported option).  \\n   - Add the document embeddings to the store.  \\n   - The vector store supports similarity search by comparing query embeddings to stored vectors.\\n\\n4. **Create a Retriever**  \\n   - Use the vector store’s retriever interface to query for passages most similar to an input text.  \\n   - The retriever returns the most relevant sections of the original documents.\\n\\n5. **(Optional) Integrate with a RAG Setup**  \\n   - Combine the retriever with a language model to perform **retrieval-augmented generation (RAG)**—a workflow where the model reasons over retrieved text to generate informed responses.\\n\\n6. **(Optional) Track and Debug with LangSmith**  \\n   - Enable LangSmith tracing by setting environment variables (`LANGSMITH_TRACING` and `LANGSMITH_API_KEY`) to log and inspect each step in your search pipeline.\\n\\nIn summary, the process involves: **loading documents → splitting text → creating embeddings → storing them in a vector store → retrieving semantically similar passages**, optionally integrated into a RAG pipeline.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"How to build a semantic search engine using LangChain?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "273cd504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To load and prepare documents for a semantic search engine using **LangChain**, you start by using the **Document** abstraction and supporting tools provided in the framework.  \\n\\n**Step 1: Load Documents**  \\nLangChain uses *document loaders* to bring text data into your environment. Each loaded document is represented by a `Document` object with three main attributes:\\n- **page_content** – the text of the document or chunk,  \\n- **metadata** – a dictionary storing contextual info (e.g., source, page numbers),  \\n- **id** – an optional unique identifier.\\n\\nYou can generate or load documents from various sources such as PDF files using the `pypdf` package:\\n```python\\nfrom langchain_core.documents import Document\\n# Example creation of a document\\ndoc = Document(page_content=\"some text\", metadata={\"source\": \"example.pdf\"})\\n```\\n\\n**Step 2: Split Text (if needed)**  \\nSince large documents can be difficult to handle as a single block of text, LangChain provides **text splitters** to divide them into smaller, meaningful chunks. Each chunk becomes its own `Document` instance, enabling better embedding and retrieval performance.\\n\\n**Step 3: Create Embeddings**  \\nNext, each document chunk is converted into numerical vector representations (embeddings) using an embedding model. These embeddings are what enable semantic similarity comparisons between queries and stored text.\\n\\n**Step 4: Store in a Vector Store**  \\nThe document embeddings are then saved into a **vector store**, which indexes and retrieves semantically similar documents efficiently.\\n\\n**In summary:**  \\n1. Load raw text or files using document loaders.  \\n2. Split text into smaller chunks using text splitters.  \\n3. Generate embeddings for each chunk.  \\n4. Store embeddings in a vector database for retrieval.  \\n\\nThese steps prepare your documents so a LangChain-based semantic search engine can find and return content similar in meaning to any user query.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"How to load and prepare documents for semantic search engine using LangChain?\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
