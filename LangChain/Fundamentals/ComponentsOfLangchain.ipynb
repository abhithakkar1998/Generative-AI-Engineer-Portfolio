{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2a1b1f",
   "metadata": {},
   "source": [
    "## LangChain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f77be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# for Langsmith tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6219ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={} client=<openai.resources.chat.completions.completions.Completions object at 0x7fd51a31c190> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd4c1bb83d0> root_client=<openai.lib.azure.AzureOpenAI object at 0x7fd4c207cfd0> root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x7fd4c1f381d0> model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True disabled_params={'parallel_tool_calls': None} azure_endpoint='https://pfs-2-namit-resource.cognitiveservices.azure.com/' deployment_name='gpt-5-chat' openai_api_version='2025-01-01-preview' openai_api_type='azure'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_LLM_MODEL\"),\n",
    "    api_version = \"2025-01-01-preview\",\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576503d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure! The theory of relativity actually refers to two related ideas proposed by Albert Einstein: **special relativity** and **general relativity**. Here‚Äôs a simple breakdown:\\n\\n### 1. Special Relativity (1905)\\nThis part deals with how space and time are connected **when objects move at constant speeds**, especially speeds close to the speed of light.\\n\\n**Key ideas:**\\n- **The speed of light is always the same** for everyone, no matter how fast they‚Äôre moving.\\n- **Time and space are not absolute:** How fast time passes and how long distances appear can change depending on how you move.\\n  \\nFor example:\\n- A clock moving very fast will tick **slower** compared to one that‚Äôs not moving, according to an outside observer. (This is called *time dilation*.)\\n- A fast-moving object will also appear **shorter** in the direction of motion. (This is called *length contraction*.)\\n\\nBasically, space and time are tied together into something called **spacetime**, and movement affects both.\\n\\n### 2. General Relativity (1915)\\nThis part extends the idea to include **gravity** and **acceleration**.\\n\\n**Key ideas:**\\n- **Gravity isn‚Äôt a force** that pulls things, like Newton thought‚Äîit‚Äôs the **curving of spacetime** caused by mass and energy.\\n- Heavy objects, like planets and stars, **bend spacetime**, and this curvature tells objects how to move.\\n\\nFor example:\\n- The Earth orbits the Sun not because the Sun pulls it with a force, but because the Sun‚Äôs mass bends spacetime around it, and Earth follows that curved path.\\n\\n### In short:\\n- **Special relativity** explains what happens when things move very fast.\\n- **General relativity** explains how gravity works as the bending of spacetime.\\n\\nTogether, they give us a new picture of the universe where space, time, energy, and matter are all connected.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 400, 'prompt_tokens': 17, 'total_tokens': 417, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-chat-2025-10-03', 'system_fingerprint': 'fp_88bf7c189b', 'id': 'chatcmpl-Cuyq4U21YYWOBqjozyeq23V9NgvOS', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019b92e8-683c-7f82-98d9-79727ad7653c-0' usage_metadata={'input_tokens': 17, 'output_tokens': 400, 'total_tokens': 417, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "##Input and get response from LLM\n",
    "\n",
    "response = llm.invoke(\"Explain the theory of relativity in simple terms.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312e987",
   "metadata": {},
   "source": [
    "##### Chatprompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41124d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question asked.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me answers based on the question asked.\"),\n",
    "    (\"user\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4450dbd",
   "metadata": {},
   "source": [
    "##### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b09e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**LangChain** is an open-source framework designed to help developers build applications powered by **large language models (LLMs)** such as OpenAI‚Äôs GPT, Anthropic‚Äôs Claude, or open-source models like Llama and Mistral.\\n\\n### üîç Core Idea\\nLangChain‚Äôs primary goal is to make LLMs **useful beyond single-prompt interactions**‚Äîby connecting them with external data sources, tools, and APIs, and by managing complex reasoning or multi-step workflows.\\n\\n---\\n\\n### üß© Key Components\\n\\n1. **LLM Wrappers**\\n   - A standardized interface to interact with different language models (from OpenAI, Anthropic, Hugging Face, etc.).\\n\\n2. **Prompt Templates**\\n   - Structures and utilities for building consistent, parameterized prompts.\\n\\n3. **Chains**\\n   - Sequences of calls (to models, APIs, or other logic) that can be combined to form complex reasoning workflows.\\n\\n4. **Memory**\\n   - Mechanisms to store and retrieve context from past interactions‚Äîenabling stateful or conversational applications.\\n\\n5. **Agents**\\n   - Components that use LLMs to decide **which actions or tools to use** in real-time (like searching databases, calling APIs, etc.).\\n\\n6. **Tools & Integrations**\\n   - Connectors to data sources and services‚Äîlike SQL databases, vector stores (Pinecone, FAISS, Chroma), APIs, and local files.\\n\\n7. **Retrieval Augmented Generation (RAG)**\\n   - Integrations for \"retrieval + generation\" pipelines where an LLM uses retrieved data (e.g., from documents or embeddings) to provide accurate responses.\\n\\n---\\n\\n### üõ† Common Use Cases\\n\\n- **Chatbots and virtual assistants**\\n- **Question answering over private data**\\n- **Document summarization and analysis**\\n- **Automation workflows with reasoning**\\n- **Conversational retrieval systems (RAG)**\\n\\n---\\n\\n### üåê Example Ecosystem\\n\\nLangChain typically works with:\\n- **Vector stores**: Pinecone, Chroma, Weaviate\\n- **LLMs**: OpenAI, Anthropic, HuggingFace models\\n- **Embeddings**: SentenceTransformers, OpenAI embeddings\\n- **Frameworks**: LangServe (for deployment), LangSmith (for debugging and tracing)\\n\\n---\\n\\n### ‚öôÔ∏è Example (Conceptual)\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings()\\ndb = Chroma(persist_directory=\"./data\", embedding_function=embeddings)\\nretriever = db.as_retriever()\\n\\nqa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\\nresponse = qa_chain.run(\"What are the main findings from the research paper?\")\\nprint(response)\\n```\\n\\n---\\n\\n### üîó Summary\\nLangChain provides a **modular** and **extensible** framework for connecting LLMs to:\\n- External knowledge,\\n- Tools,\\n- Custom logic,\\n- And memory.\\n\\nIt‚Äôs especially valuable when you want to move beyond simple chat completion APIs and build **production-grade AI applications** that reason, remember, and integrate with the real world.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 673, 'prompt_tokens': 32, 'total_tokens': 705, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-chat-2025-10-03', 'system_fingerprint': 'fp_88bf7c189b', 'id': 'chatcmpl-CuywO3aqY1wng4TPHAIqgG6m8sNfT', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019b92ee-5f2b-78d2-87ab-573dcd419b23-0' usage_metadata={'input_tokens': 32, 'output_tokens': 673, 'total_tokens': 705, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"user_input\": \"What is LangChain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54534d1b",
   "metadata": {},
   "source": [
    "##### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9271c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f169ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke({\"user_input\": \"Explain quantum computing in simple terms.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7754959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let‚Äôs break down **quantum computing** in simple terms:\n",
      "\n",
      "---\n",
      "\n",
      "### üß† The Big Idea\n",
      "Traditional computers use **bits**, which can be either a **0** or **1**.  \n",
      "Quantum computers use **qubits (quantum bits)**, which can be **0**, **1**, or **both at the same time** (this is called **superposition**).\n",
      "\n",
      "---\n",
      "\n",
      "### ‚ö° Superposition\n",
      "Imagine a light switch:\n",
      "- A classical bit is like a switch that‚Äôs **on (1)** or **off (0)**.\n",
      "- A qubit is like a **dimmer switch**, which can be **any combination** of on and off at once.  \n",
      "This means quantum computers can explore **many possibilities simultaneously**.\n",
      "\n",
      "---\n",
      "\n",
      "### üîó Entanglement\n",
      "When qubits are **entangled**, the state of one qubit depends on another.  \n",
      "It‚Äôs like having two dice that are always linked ‚Äî if you roll one and get a 6, the other automatically adjusts in response.  \n",
      "This allows quantum computers to process **complex relationships** between data very efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "### üåÄ Interference\n",
      "Quantum computers use **interference** to **amplify** the correct answers and **cancel out** wrong ones ‚Äî sort of like adjusting sound waves so only the right notes come through clearly.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚öôÔ∏è Why It Matters\n",
      "Quantum computers have the potential to solve some problems **much faster** than classical computers:\n",
      "- **Breaking encryption**\n",
      "- **Modeling molecules** for drug discovery\n",
      "- **Optimizing complex systems** (like traffic or supply chains)\n",
      "- **AI and machine learning speed-ups**\n",
      "\n",
      "---\n",
      "\n",
      "### üß© The Catch\n",
      "Quantum computing is still in its early stages ‚Äî qubits are **fragile** and easily disturbed by their environment.  \n",
      "Researchers are still figuring out how to make systems stable and large enough to do practical tasks.\n",
      "\n",
      "---\n",
      "\n",
      "**In short:**  \n",
      "A **classical computer** tries possible answers one by one.  \n",
      "A **quantum computer** can explore **many possibilities at once** ‚Äî which is why it could someday outperform even the fastest supercomputers on specific types of problems.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c65dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
