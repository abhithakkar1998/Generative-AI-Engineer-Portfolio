{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648d100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/abhi/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a25d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "with open('macbeth.txt', 'w') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b64653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 16:52:09.804957: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 16:52:09.848976: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 16:52:10.973122: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "## Data preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a948be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3553"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('macbeth.txt', 'r') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea49bdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'i': 5,\n",
       " 'a': 6,\n",
       " 'that': 7,\n",
       " 'my': 8,\n",
       " 'you': 9,\n",
       " 'in': 10,\n",
       " 'is': 11,\n",
       " 'not': 12,\n",
       " 'it': 13,\n",
       " 'with': 14,\n",
       " 'his': 15,\n",
       " 'be': 16,\n",
       " 'macb': 17,\n",
       " 'your': 18,\n",
       " 'our': 19,\n",
       " 'haue': 20,\n",
       " 'but': 21,\n",
       " 'me': 22,\n",
       " 'he': 23,\n",
       " 'for': 24,\n",
       " 'what': 25,\n",
       " 'this': 26,\n",
       " 'all': 27,\n",
       " 'so': 28,\n",
       " 'him': 29,\n",
       " 'as': 30,\n",
       " 'thou': 31,\n",
       " 'we': 32,\n",
       " 'enter': 33,\n",
       " 'which': 34,\n",
       " 'are': 35,\n",
       " 'will': 36,\n",
       " 'they': 37,\n",
       " 'shall': 38,\n",
       " 'no': 39,\n",
       " 'then': 40,\n",
       " 'macbeth': 41,\n",
       " 'their': 42,\n",
       " 'thee': 43,\n",
       " 'vpon': 44,\n",
       " 'on': 45,\n",
       " 'macd': 46,\n",
       " 'from': 47,\n",
       " 'yet': 48,\n",
       " 'thy': 49,\n",
       " 'vs': 50,\n",
       " 'come': 51,\n",
       " 'king': 52,\n",
       " 'now': 53,\n",
       " 'at': 54,\n",
       " 'hath': 55,\n",
       " 'more': 56,\n",
       " 'by': 57,\n",
       " 'good': 58,\n",
       " 'rosse': 59,\n",
       " 'them': 60,\n",
       " 'lady': 61,\n",
       " 'would': 62,\n",
       " 'time': 63,\n",
       " 'was': 64,\n",
       " 'do': 65,\n",
       " 'who': 66,\n",
       " 'like': 67,\n",
       " 'her': 68,\n",
       " 'if': 69,\n",
       " 'should': 70,\n",
       " 'did': 71,\n",
       " 'when': 72,\n",
       " 'there': 73,\n",
       " 'say': 74,\n",
       " 'were': 75,\n",
       " 'where': 76,\n",
       " 'doe': 77,\n",
       " 'lord': 78,\n",
       " 'make': 79,\n",
       " 'or': 80,\n",
       " '1': 81,\n",
       " 'must': 82,\n",
       " \"'tis\": 83,\n",
       " 'done': 84,\n",
       " 'selfe': 85,\n",
       " 'ile': 86,\n",
       " 'feare': 87,\n",
       " 'let': 88,\n",
       " 'man': 89,\n",
       " 'had': 90,\n",
       " 'wife': 91,\n",
       " 'night': 92,\n",
       " 'banquo': 93,\n",
       " 'how': 94,\n",
       " 'well': 95,\n",
       " 'know': 96,\n",
       " 'why': 97,\n",
       " 'one': 98,\n",
       " 'great': 99,\n",
       " 'see': 100,\n",
       " 'may': 101,\n",
       " 'exeunt': 102,\n",
       " 'too': 103,\n",
       " 'am': 104,\n",
       " 'speake': 105,\n",
       " 'sir': 106,\n",
       " 'lenox': 107,\n",
       " 'an': 108,\n",
       " 'out': 109,\n",
       " 'mine': 110,\n",
       " 'vp': 111,\n",
       " \"th'\": 112,\n",
       " 'can': 113,\n",
       " 'mal': 114,\n",
       " 'thane': 115,\n",
       " 'nor': 116,\n",
       " 'looke': 117,\n",
       " 'giue': 118,\n",
       " 'banq': 119,\n",
       " 'such': 120,\n",
       " 'those': 121,\n",
       " 'blood': 122,\n",
       " 'these': 123,\n",
       " 'things': 124,\n",
       " 'most': 125,\n",
       " 'sleepe': 126,\n",
       " 'hand': 127,\n",
       " '2': 128,\n",
       " 'scena': 129,\n",
       " 'heere': 130,\n",
       " 'againe': 131,\n",
       " 'cawdor': 132,\n",
       " 'before': 133,\n",
       " 'la': 134,\n",
       " '3': 135,\n",
       " 'nature': 136,\n",
       " 'till': 137,\n",
       " 'cannot': 138,\n",
       " 'death': 139,\n",
       " 'doct': 140,\n",
       " 'day': 141,\n",
       " 'loue': 142,\n",
       " 'still': 143,\n",
       " 'art': 144,\n",
       " 'both': 145,\n",
       " 'life': 146,\n",
       " 'heart': 147,\n",
       " 'heare': 148,\n",
       " 'macduffe': 149,\n",
       " 'within': 150,\n",
       " 'o': 151,\n",
       " 'men': 152,\n",
       " 'here': 153,\n",
       " 'though': 154,\n",
       " 'way': 155,\n",
       " \"ha's\": 156,\n",
       " 'owne': 157,\n",
       " 'take': 158,\n",
       " 'she': 159,\n",
       " 'some': 160,\n",
       " 'sey': 161,\n",
       " 'worthy': 162,\n",
       " 'strange': 163,\n",
       " 'shew': 164,\n",
       " 'nothing': 165,\n",
       " 'euery': 166,\n",
       " 'knock': 167,\n",
       " 'put': 168,\n",
       " 'poore': 169,\n",
       " \"do's\": 170,\n",
       " 'son': 171,\n",
       " 'ere': 172,\n",
       " 'ayre': 173,\n",
       " 'bloody': 174,\n",
       " 'goe': 175,\n",
       " 'other': 176,\n",
       " \"i'th'\": 177,\n",
       " 'downe': 178,\n",
       " 'thought': 179,\n",
       " 'heauen': 180,\n",
       " 'borne': 181,\n",
       " 'malc': 182,\n",
       " 'haile': 183,\n",
       " 'leaue': 184,\n",
       " 'name': 185,\n",
       " 'god': 186,\n",
       " 'against': 187,\n",
       " 'without': 188,\n",
       " 'whose': 189,\n",
       " \"there's\": 190,\n",
       " 'dead': 191,\n",
       " 'euer': 192,\n",
       " 'could': 193,\n",
       " 'deed': 194,\n",
       " 'father': 195,\n",
       " 'three': 196,\n",
       " 'new': 197,\n",
       " 'comes': 198,\n",
       " 'noble': 199,\n",
       " \"o'th'\": 200,\n",
       " 'thus': 201,\n",
       " 'about': 202,\n",
       " 'each': 203,\n",
       " 'much': 204,\n",
       " 'none': 205,\n",
       " 'pray': 206,\n",
       " 'thinke': 207,\n",
       " 'malcolme': 208,\n",
       " 'made': 209,\n",
       " 'exit': 210,\n",
       " 'neuer': 211,\n",
       " 'dare': 212,\n",
       " 'sword': 213,\n",
       " 'hands': 214,\n",
       " 'scotland': 215,\n",
       " 'into': 216,\n",
       " 'beare': 217,\n",
       " 'onely': 218,\n",
       " 'euen': 219,\n",
       " 'eye': 220,\n",
       " 'keepe': 221,\n",
       " 'bed': 222,\n",
       " 'first': 223,\n",
       " \"what's\": 224,\n",
       " 'place': 225,\n",
       " 'two': 226,\n",
       " 'double': 227,\n",
       " 'honor': 228,\n",
       " 'eyes': 229,\n",
       " 'beene': 230,\n",
       " 'very': 231,\n",
       " 'liue': 232,\n",
       " 'peace': 233,\n",
       " 'once': 234,\n",
       " 'kings': 235,\n",
       " 'liues': 236,\n",
       " 'away': 237,\n",
       " 'finde': 238,\n",
       " 'face': 239,\n",
       " 'whom': 240,\n",
       " 'welcome': 241,\n",
       " 'full': 242,\n",
       " 'hold': 243,\n",
       " 'hence': 244,\n",
       " 'else': 245,\n",
       " 'hell': 246,\n",
       " 'morrow': 247,\n",
       " 'seruant': 248,\n",
       " 'stand': 249,\n",
       " 'fight': 250,\n",
       " 'go': 251,\n",
       " 'wood': 252,\n",
       " 'gent': 253,\n",
       " 'tell': 254,\n",
       " 'words': 255,\n",
       " 'thine': 256,\n",
       " 'mac': 257,\n",
       " 'grace': 258,\n",
       " 'sight': 259,\n",
       " 'call': 260,\n",
       " 'houre': 261,\n",
       " 'friends': 262,\n",
       " 'being': 263,\n",
       " 'duncan': 264,\n",
       " 'thoughts': 265,\n",
       " 'old': 266,\n",
       " 'flye': 267,\n",
       " 'seyward': 268,\n",
       " 'woman': 269,\n",
       " 'thunder': 270,\n",
       " 'report': 271,\n",
       " 'head': 272,\n",
       " 'cry': 273,\n",
       " 'get': 274,\n",
       " 'house': 275,\n",
       " 'royall': 276,\n",
       " 'hope': 277,\n",
       " 'rest': 278,\n",
       " 'lesse': 279,\n",
       " 'murther': 280,\n",
       " 'highnesse': 281,\n",
       " 'further': 282,\n",
       " 'light': 283,\n",
       " 'false': 284,\n",
       " 'tongue': 285,\n",
       " \"that's\": 286,\n",
       " 'businesse': 287,\n",
       " 'macduff': 288,\n",
       " 'off': 289,\n",
       " 'bring': 290,\n",
       " 'last': 291,\n",
       " 'minde': 292,\n",
       " 'lye': 293,\n",
       " 'many': 294,\n",
       " 'murth': 295,\n",
       " 'fleans': 296,\n",
       " 'tyrant': 297,\n",
       " 'witches': 298,\n",
       " 'set': 299,\n",
       " 'hee': 300,\n",
       " 'himselfe': 301,\n",
       " 'fell': 302,\n",
       " 'present': 303,\n",
       " 'hast': 304,\n",
       " 'gone': 305,\n",
       " 'hang': 306,\n",
       " \"is't\": 307,\n",
       " 'earth': 308,\n",
       " 'truth': 309,\n",
       " 'children': 310,\n",
       " 'true': 311,\n",
       " 'vnder': 312,\n",
       " 'giuen': 313,\n",
       " 'feares': 314,\n",
       " 'enough': 315,\n",
       " 'thing': 316,\n",
       " 'knowne': 317,\n",
       " 'sorrow': 318,\n",
       " 'rather': 319,\n",
       " 'almost': 320,\n",
       " 'nights': 321,\n",
       " 'dayes': 322,\n",
       " 'does': 323,\n",
       " 'heard': 324,\n",
       " 'drinke': 325,\n",
       " 'bell': 326,\n",
       " 'knocking': 327,\n",
       " 'best': 328,\n",
       " 'lords': 329,\n",
       " 'any': 330,\n",
       " 'england': 331,\n",
       " 'better': 332,\n",
       " 'sonne': 333,\n",
       " 'doctor': 334,\n",
       " 'dunsinane': 335,\n",
       " 'meet': 336,\n",
       " 'anon': 337,\n",
       " 'donalbaine': 338,\n",
       " 'state': 339,\n",
       " 'fortune': 340,\n",
       " 'sisters': 341,\n",
       " 'seeme': 342,\n",
       " 'breath': 343,\n",
       " \"who's\": 344,\n",
       " 'might': 345,\n",
       " 'throw': 346,\n",
       " \"let's\": 347,\n",
       " 'bid': 348,\n",
       " 'desire': 349,\n",
       " 'comming': 350,\n",
       " 'shake': 351,\n",
       " 'goes': 352,\n",
       " 'trouble': 353,\n",
       " 'while': 354,\n",
       " 'daggers': 355,\n",
       " 'oh': 356,\n",
       " 'ban': 357,\n",
       " 'fled': 358,\n",
       " 'sit': 359,\n",
       " 'gracious': 360,\n",
       " 'power': 361,\n",
       " \"wee'l\": 362,\n",
       " 'mur': 363,\n",
       " 'knowes': 364,\n",
       " 'lost': 365,\n",
       " 'faire': 366,\n",
       " 'through': 367,\n",
       " 'attendants': 368,\n",
       " 'whence': 369,\n",
       " 'another': 370,\n",
       " 'helpe': 371,\n",
       " 'angus': 372,\n",
       " 'cold': 373,\n",
       " 'vse': 374,\n",
       " 'master': 375,\n",
       " 'doth': 376,\n",
       " 'seene': 377,\n",
       " 'shalt': 378,\n",
       " 'withall': 379,\n",
       " \"o're\": 380,\n",
       " 'word': 381,\n",
       " 'forth': 382,\n",
       " 'deepe': 383,\n",
       " 'left': 384,\n",
       " 'themselues': 385,\n",
       " 'thanes': 386,\n",
       " 'fall': 387,\n",
       " 'lyes': 388,\n",
       " 'after': 389,\n",
       " 'mortall': 390,\n",
       " \"would'st\": 391,\n",
       " 'powre': 392,\n",
       " 'round': 393,\n",
       " 'fate': 394,\n",
       " 'messenger': 395,\n",
       " 'please': 396,\n",
       " 'natures': 397,\n",
       " 'makes': 398,\n",
       " 'gentle': 399,\n",
       " 'late': 400,\n",
       " 'chamber': 401,\n",
       " 'since': 402,\n",
       " 'world': 403,\n",
       " 'hearke': 404,\n",
       " 'little': 405,\n",
       " 'english': 406,\n",
       " 'command': 407,\n",
       " 'thence': 408,\n",
       " 'rise': 409,\n",
       " 'colours': 410,\n",
       " 'neere': 411,\n",
       " 'issue': 412,\n",
       " 'send': 413,\n",
       " 'wisedome': 414,\n",
       " 'tyrants': 415,\n",
       " 'y': 416,\n",
       " 'seyton': 417,\n",
       " 'actus': 418,\n",
       " 'prima': 419,\n",
       " 'foule': 420,\n",
       " 'secunda': 421,\n",
       " 'alarum': 422,\n",
       " 'meeting': 423,\n",
       " 'friend': 424,\n",
       " 'together': 425,\n",
       " 'bad': 426,\n",
       " \"toth'\": 427,\n",
       " 'comfort': 428,\n",
       " 'ouer': 429,\n",
       " 'seemes': 430,\n",
       " 'terrible': 431,\n",
       " 'point': 432,\n",
       " 'ten': 433,\n",
       " 'former': 434,\n",
       " 'title': 435,\n",
       " 'tertia': 436,\n",
       " 'thither': 437,\n",
       " 'thrice': 438,\n",
       " 'farre': 439,\n",
       " \"call'd\": 440,\n",
       " 'women': 441,\n",
       " 'sound': 442,\n",
       " 'indeed': 443,\n",
       " 'stay': 444,\n",
       " 'stands': 445,\n",
       " 'water': 446,\n",
       " 'takes': 447,\n",
       " 'reason': 448,\n",
       " 'thankes': 449,\n",
       " 'home': 450,\n",
       " 'crowne': 451,\n",
       " 'harme': 452,\n",
       " 'honest': 453,\n",
       " 'chance': 454,\n",
       " 'braine': 455,\n",
       " 'toward': 456,\n",
       " 'free': 457,\n",
       " 'flourish': 458,\n",
       " 'saw': 459,\n",
       " 'part': 460,\n",
       " 'receiue': 461,\n",
       " 'duties': 462,\n",
       " 'safe': 463,\n",
       " 'hither': 464,\n",
       " 'sonnes': 465,\n",
       " 'care': 466,\n",
       " 'came': 467,\n",
       " 'lay': 468,\n",
       " 'attend': 469,\n",
       " 'high': 470,\n",
       " 'spirits': 471,\n",
       " 'eare': 472,\n",
       " 'fill': 473,\n",
       " 'purpose': 474,\n",
       " 'darke': 475,\n",
       " 'innocent': 476,\n",
       " 'hostesse': 477,\n",
       " 'pleasure': 478,\n",
       " 'returne': 479,\n",
       " \"hee's\": 480,\n",
       " 'strong': 481,\n",
       " 'pale': 482,\n",
       " 'strike': 483,\n",
       " 'horror': 484,\n",
       " 'bold': 485,\n",
       " 'fire': 486,\n",
       " \"'em\": 487,\n",
       " 'amen': 488,\n",
       " 'feast': 489,\n",
       " \"murther'd\": 490,\n",
       " 'faces': 491,\n",
       " 'porter': 492,\n",
       " 'said': 493,\n",
       " 'selues': 494,\n",
       " 'horses': 495,\n",
       " 'certaine': 496,\n",
       " 'slaine': 497,\n",
       " 'meanes': 498,\n",
       " 'bene': 499,\n",
       " 'graue': 500,\n",
       " 'cause': 501,\n",
       " \"banquo's\": 502,\n",
       " 'worst': 503,\n",
       " 'health': 504,\n",
       " 'backe': 505,\n",
       " 'len': 506,\n",
       " 'answer': 507,\n",
       " 'country': 508,\n",
       " 'cauldron': 509,\n",
       " 'mother': 510,\n",
       " 'soldiers': 511,\n",
       " 'ment': 512,\n",
       " 'souldier': 513,\n",
       " \"'gainst\": 514,\n",
       " 'stood': 515,\n",
       " \"all's\": 516,\n",
       " 'valiant': 517,\n",
       " 'valour': 518,\n",
       " 'trust': 519,\n",
       " 'become': 520,\n",
       " 'people': 521,\n",
       " 'arme': 522,\n",
       " 'thousand': 523,\n",
       " 'generall': 524,\n",
       " 'bosome': 525,\n",
       " 'pronounce': 526,\n",
       " 'winde': 527,\n",
       " 'blow': 528,\n",
       " 'mans': 529,\n",
       " 'nine': 530,\n",
       " 'times': 531,\n",
       " 'drum': 532,\n",
       " 'drumme': 533,\n",
       " \"on't\": 534,\n",
       " 'question': 535,\n",
       " 'glamis': 536,\n",
       " 'start': 537,\n",
       " 'hauing': 538,\n",
       " 'owe': 539,\n",
       " 'went': 540,\n",
       " 'newes': 541,\n",
       " 'successe': 542,\n",
       " 'ang': 543,\n",
       " 'wee': 544,\n",
       " 'sent': 545,\n",
       " 'thanks': 546,\n",
       " 'pay': 547,\n",
       " 'deuill': 548,\n",
       " 'heauie': 549,\n",
       " 'whether': 550,\n",
       " 'glamys': 551,\n",
       " 'paines': 552,\n",
       " \"promis'd\": 553,\n",
       " 'act': 554,\n",
       " 'thanke': 555,\n",
       " 'gentlemen': 556,\n",
       " 'ill': 557,\n",
       " 'yeeld': 558,\n",
       " 'honors': 559,\n",
       " 'wrought': 560,\n",
       " 'quarta': 561,\n",
       " 'spoke': 562,\n",
       " \"dy'de\": 563,\n",
       " 'dearest': 564,\n",
       " 'due': 565,\n",
       " 'seruice': 566,\n",
       " 'throne': 567,\n",
       " 'seruants': 568,\n",
       " 'hide': 569,\n",
       " 'approach': 570,\n",
       " 'black': 571,\n",
       " 'macbeths': 572,\n",
       " 'alone': 573,\n",
       " 'whiles': 574,\n",
       " 'play': 575,\n",
       " 'golden': 576,\n",
       " 'fatall': 577,\n",
       " 'feele': 578,\n",
       " 'castle': 579,\n",
       " 'heauens': 580,\n",
       " 'breed': 581,\n",
       " 'graces': 582,\n",
       " 'towards': 583,\n",
       " 'end': 584,\n",
       " 'sides': 585,\n",
       " 'prythee': 586,\n",
       " \"was't\": 587,\n",
       " 'performe': 588,\n",
       " 'fleance': 589,\n",
       " 'giues': 590,\n",
       " 'spend': 591,\n",
       " 'worth': 592,\n",
       " 'walke': 593,\n",
       " 'owle': 594,\n",
       " 'dye': 595,\n",
       " 'husband': 596,\n",
       " 'second': 597,\n",
       " 'laugh': 598,\n",
       " \"cry'd\": 599,\n",
       " 'need': 600,\n",
       " 'meane': 601,\n",
       " 'bleed': 602,\n",
       " 'knocke': 603,\n",
       " 'shame': 604,\n",
       " 'least': 605,\n",
       " \"here's\": 606,\n",
       " 'gate': 607,\n",
       " 'faith': 608,\n",
       " 'remember': 609,\n",
       " 'long': 610,\n",
       " 'young': 611,\n",
       " 'broke': 612,\n",
       " 'ring': 613,\n",
       " 'alas': 614,\n",
       " 'cruell': 615,\n",
       " \"liu'd\": 616,\n",
       " 'violent': 617,\n",
       " 'murtherers': 618,\n",
       " 'worke': 619,\n",
       " 'horse': 620,\n",
       " \"kill'd\": 621,\n",
       " 'swift': 622,\n",
       " 'warre': 623,\n",
       " 'eate': 624,\n",
       " 'fife': 625,\n",
       " \"heere's\": 626,\n",
       " 'sure': 627,\n",
       " 'enemie': 628,\n",
       " 'soule': 629,\n",
       " 'perfect': 630,\n",
       " 'loues': 631,\n",
       " 'something': 632,\n",
       " 'keepes': 633,\n",
       " 'soules': 634,\n",
       " 'flight': 635,\n",
       " 'court': 636,\n",
       " 'note': 637,\n",
       " 'scaena': 638,\n",
       " 'cut': 639,\n",
       " 'fit': 640,\n",
       " 'whole': 641,\n",
       " 'cheere': 642,\n",
       " 'often': 643,\n",
       " 'sweet': 644,\n",
       " 'diuell': 645,\n",
       " 'behold': 646,\n",
       " 'worse': 647,\n",
       " 'moue': 648,\n",
       " 'yong': 649,\n",
       " 'scorne': 650,\n",
       " 'burne': 651,\n",
       " 'bubble': 652,\n",
       " 'charme': 653,\n",
       " 'blacke': 654,\n",
       " 'speech': 655,\n",
       " 'appar': 656,\n",
       " 'childe': 657,\n",
       " 'hart': 658,\n",
       " 'traitor': 659,\n",
       " 'mes': 660,\n",
       " 'doubt': 661,\n",
       " 'villaine': 662,\n",
       " 'griefe': 663,\n",
       " 'byrnane': 664,\n",
       " 'alarums': 665,\n",
       " 'sunne': 666,\n",
       " 'newest': 667,\n",
       " 'knowledge': 668,\n",
       " 'didst': 669,\n",
       " 'cap': 670,\n",
       " 'kernes': 671,\n",
       " 'damned': 672,\n",
       " 'execution': 673,\n",
       " \"fac'd\": 674,\n",
       " 'slaue': 675,\n",
       " 'gentleman': 676,\n",
       " \"seem'd\": 677,\n",
       " 'marke': 678,\n",
       " 'iustice': 679,\n",
       " 'norweyan': 680,\n",
       " 'vantage': 681,\n",
       " 'began': 682,\n",
       " \"charg'd\": 683,\n",
       " 'wounds': 684,\n",
       " 'gashes': 685,\n",
       " 'dismall': 686,\n",
       " 'spirit': 687,\n",
       " 'mouncht': 688,\n",
       " 'kinde': 689,\n",
       " 'weyward': 690,\n",
       " 'sea': 691,\n",
       " 'land': 692,\n",
       " 'wilde': 693,\n",
       " 'lips': 694,\n",
       " 'hereafter': 695,\n",
       " 'ye': 696,\n",
       " 'hayle': 697,\n",
       " 'greater': 698,\n",
       " 'happy': 699,\n",
       " 'charge': 700,\n",
       " 'same': 701,\n",
       " 'contend': 702,\n",
       " 'thick': 703,\n",
       " 'loose': 704,\n",
       " 'vnto': 705,\n",
       " 'besides': 706,\n",
       " 'winne': 707,\n",
       " 'instruments': 708,\n",
       " 'horrid': 709,\n",
       " 'horrible': 710,\n",
       " 'single': 711,\n",
       " 'leysure': 712,\n",
       " 'turne': 713,\n",
       " 'hearts': 714,\n",
       " 'liege': 715,\n",
       " \"'twere\": 716,\n",
       " 'mindes': 717,\n",
       " 'absolute': 718,\n",
       " 'wing': 719,\n",
       " 'labour': 720,\n",
       " 'seeke': 721,\n",
       " 'shine': 722,\n",
       " 'quinta': 723,\n",
       " 'met': 724,\n",
       " 'greatnesse': 725,\n",
       " 'milke': 726,\n",
       " 'catch': 727,\n",
       " 'ambition': 728,\n",
       " 'mad': 729,\n",
       " 'top': 730,\n",
       " 'womans': 731,\n",
       " 'sexta': 732,\n",
       " 'hoboyes': 733,\n",
       " 'sences': 734,\n",
       " 'guest': 735,\n",
       " 'bird': 736,\n",
       " 'sometime': 737,\n",
       " 'broad': 738,\n",
       " 'host': 739,\n",
       " 'stage': 740,\n",
       " 'quickly': 741,\n",
       " 'shut': 742,\n",
       " 'doore': 743,\n",
       " 'duncane': 744,\n",
       " 'bin': 745,\n",
       " 'office': 746,\n",
       " 'taking': 747,\n",
       " 'babe': 748,\n",
       " 'ore': 749,\n",
       " 'cast': 750,\n",
       " 'soone': 751,\n",
       " \"affear'd\": 752,\n",
       " 'dares': 753,\n",
       " 'breake': 754,\n",
       " 'faile': 755,\n",
       " 'courage': 756,\n",
       " 'hard': 757,\n",
       " 'wine': 758,\n",
       " \"don't\": 759,\n",
       " 'torch': 760,\n",
       " 'boy': 761,\n",
       " 'summons': 762,\n",
       " 'ready': 763,\n",
       " 'dagger': 764,\n",
       " 'draw': 765,\n",
       " 'halfe': 766,\n",
       " 'watch': 767,\n",
       " 'ghost': 768,\n",
       " 'firme': 769,\n",
       " 'knell': 770,\n",
       " 'm': 771,\n",
       " 'open': 772,\n",
       " 'groomes': 773,\n",
       " 'hoa': 774,\n",
       " 'misse': 775,\n",
       " 'noyse': 776,\n",
       " 'wake': 777,\n",
       " 'prayers': 778,\n",
       " 'blesse': 779,\n",
       " 'wherefore': 780,\n",
       " 'blessing': 781,\n",
       " 'throat': 782,\n",
       " 'wayes': 783,\n",
       " 'sleep': 784,\n",
       " 'therefore': 785,\n",
       " 'strength': 786,\n",
       " 'sickly': 787,\n",
       " 'wash': 788,\n",
       " 'making': 789,\n",
       " 'weare': 790,\n",
       " 'easie': 791,\n",
       " \"hang'd\": 792,\n",
       " \"for't\": 793,\n",
       " 'equiuocator': 794,\n",
       " 'treason': 795,\n",
       " 'gods': 796,\n",
       " 'port': 797,\n",
       " 'dis': 798,\n",
       " 'beleeue': 799,\n",
       " 'tooke': 800,\n",
       " 'been': 801,\n",
       " 'remembrance': 802,\n",
       " 'awake': 803,\n",
       " 'sprights': 804,\n",
       " 'meere': 805,\n",
       " 'found': 806,\n",
       " 'kill': 807,\n",
       " 'moment': 808,\n",
       " \"look'd\": 809,\n",
       " 'spoken': 810,\n",
       " 'foot': 811,\n",
       " 'suffer': 812,\n",
       " 'mallice': 813,\n",
       " 'vnnaturall': 814,\n",
       " 'duncans': 815,\n",
       " 'obedience': 816,\n",
       " 'already': 817,\n",
       " 'scone': 818,\n",
       " 'body': 819,\n",
       " 'bones': 820,\n",
       " 'weyard': 821,\n",
       " 'roote': 822,\n",
       " 'speeches': 823,\n",
       " 'supper': 824,\n",
       " 'ride': 825,\n",
       " 'wish': 826,\n",
       " 'pallace': 827,\n",
       " 'line': 828,\n",
       " \"if't\": 829,\n",
       " 'patience': 830,\n",
       " 'yours': 831,\n",
       " 'file': 832,\n",
       " 'drop': 833,\n",
       " 'sundry': 834,\n",
       " 'close': 835,\n",
       " 'touch': 836,\n",
       " 'applaud': 837,\n",
       " 'iust': 838,\n",
       " \"may'st\": 839,\n",
       " 'maiesty': 840,\n",
       " 'bound': 841,\n",
       " 'twenty': 842,\n",
       " 'countries': 843,\n",
       " 'person': 844,\n",
       " 'murthers': 845,\n",
       " 'lacke': 846,\n",
       " 'deere': 847,\n",
       " 'aliue': 848,\n",
       " 'shadow': 849,\n",
       " 'sights': 850,\n",
       " 'growes': 851,\n",
       " 'goodnight': 852,\n",
       " 'hecat': 853,\n",
       " 'others': 854,\n",
       " 'hopes': 855,\n",
       " 'musicke': 856,\n",
       " 'right': 857,\n",
       " 'holy': 858,\n",
       " 'warlike': 859,\n",
       " 'aboue': 860,\n",
       " 'boyle': 861,\n",
       " 'castles': 862,\n",
       " 'tree': 863,\n",
       " \"damn'd\": 864,\n",
       " 'babes': 865,\n",
       " 'foole': 866,\n",
       " 'traitors': 867,\n",
       " 'birds': 868,\n",
       " 'ones': 869,\n",
       " 'pretty': 870,\n",
       " \"speak'st\": 871,\n",
       " 'fast': 872,\n",
       " 'queene': 873,\n",
       " 'cure': 874,\n",
       " 'disease': 875,\n",
       " 'remoue': 876,\n",
       " 'affraid': 877,\n",
       " 'front': 878,\n",
       " 'fiend': 879,\n",
       " 'lad': 880,\n",
       " 'cath': 881,\n",
       " 'marching': 882,\n",
       " 'ser': 883,\n",
       " 'souldiers': 884,\n",
       " 'tragedie': 885,\n",
       " 'lightning': 886,\n",
       " 'wonne': 887,\n",
       " 'heath': 888,\n",
       " 'calls': 889,\n",
       " 'filthie': 890,\n",
       " 'bleeding': 891,\n",
       " 'reuolt': 892,\n",
       " 'fought': 893,\n",
       " 'braue': 894,\n",
       " 'doubtfull': 895,\n",
       " 'spent': 896,\n",
       " 'cling': 897,\n",
       " 'rebell': 898,\n",
       " 'quarry': 899,\n",
       " \"shew'd\": 900,\n",
       " 'weake': 901,\n",
       " 'deserues': 902,\n",
       " 'steele': 903,\n",
       " 'passage': 904,\n",
       " 'farwell': 905,\n",
       " 'battlements': 906,\n",
       " 'cousin': 907,\n",
       " 'spring': 908,\n",
       " 'discomfort': 909,\n",
       " 'sooner': 910,\n",
       " \"arm'd\": 911,\n",
       " 'heeles': 912,\n",
       " 'armes': 913,\n",
       " 'yes': 914,\n",
       " 'lyon': 915,\n",
       " 'sooth': 916,\n",
       " 'stroakes': 917,\n",
       " 'lookes': 918,\n",
       " 'banners': 919,\n",
       " 'norway': 920,\n",
       " 'numbers': 921,\n",
       " 'rebellious': 922,\n",
       " 'greet': 923,\n",
       " 'sister': 924,\n",
       " 'fed': 925,\n",
       " 'cryes': 926,\n",
       " 'tiger': 927,\n",
       " 'neyther': 928,\n",
       " 'forbid': 929,\n",
       " 'wearie': 930,\n",
       " 'pine': 931,\n",
       " 'wound': 932,\n",
       " \"wither'd\": 933,\n",
       " 'finger': 934,\n",
       " 'fantasticall': 935,\n",
       " 'partner': 936,\n",
       " 'seedes': 937,\n",
       " 'grow': 938,\n",
       " 'begge': 939,\n",
       " 'hate': 940,\n",
       " 'lesser': 941,\n",
       " 'prosperous': 942,\n",
       " 'beleefe': 943,\n",
       " 'stop': 944,\n",
       " 'vanish': 945,\n",
       " 'whither': 946,\n",
       " \"vanish'd\": 947,\n",
       " 'corporall': 948,\n",
       " 'eaten': 949,\n",
       " 'root': 950,\n",
       " \"receiu'd\": 951,\n",
       " 'prayses': 952,\n",
       " 'findes': 953,\n",
       " 'tale': 954,\n",
       " 'post': 955,\n",
       " 'kingdomes': 956,\n",
       " 'defence': 957,\n",
       " 'earnest': 958,\n",
       " 'addition': 959,\n",
       " 'robes': 960,\n",
       " 'iudgement': 961,\n",
       " 'beares': 962,\n",
       " 'wracke': 963,\n",
       " 'treasons': 964,\n",
       " \"confess'd\": 965,\n",
       " 'greatest': 966,\n",
       " 'gaue': 967,\n",
       " 'trusted': 968,\n",
       " 'darknesse': 969,\n",
       " 'truths': 970,\n",
       " 'consequence': 971,\n",
       " 'told': 972,\n",
       " 'imperiall': 973,\n",
       " 'image': 974,\n",
       " 'vnfixe': 975,\n",
       " 'rapt': 976,\n",
       " 'stirre': 977,\n",
       " 'cleaue': 978,\n",
       " 'forgotten': 979,\n",
       " 'leafe': 980,\n",
       " 'reade': 981,\n",
       " \"weigh'd\": 982,\n",
       " 'pardon': 983,\n",
       " 'became': 984,\n",
       " \"ow'd\": 985,\n",
       " 'sinne': 986,\n",
       " 'slow': 987,\n",
       " \"deseru'd\": 988,\n",
       " 'doing': 989,\n",
       " 'begun': 990,\n",
       " 'plant': 991,\n",
       " 'kinsmen': 992,\n",
       " 'estate': 993,\n",
       " 'prince': 994,\n",
       " 'cumberland': 995,\n",
       " 'starres': 996,\n",
       " \"vs'd\": 997,\n",
       " 'ioyfull': 998,\n",
       " 'hearing': 999,\n",
       " 'bee': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257e82a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 885],\n",
       " [1, 885, 4],\n",
       " [1, 885, 4, 41],\n",
       " [1, 885, 4, 41, 57],\n",
       " [1, 885, 4, 41, 57, 1388],\n",
       " [1, 885, 4, 41, 57, 1388, 1389],\n",
       " [1, 885, 4, 41, 57, 1388, 1389, 1390],\n",
       " [418, 1391],\n",
       " [418, 1391, 1392],\n",
       " [418, 1391, 1392, 419],\n",
       " [270, 2],\n",
       " [270, 2, 886],\n",
       " [270, 2, 886, 33],\n",
       " [270, 2, 886, 33, 196],\n",
       " [270, 2, 886, 33, 196, 298],\n",
       " [81, 72],\n",
       " [81, 72, 38],\n",
       " [81, 72, 38, 32],\n",
       " [81, 72, 38, 32, 196],\n",
       " [81, 72, 38, 32, 196, 336],\n",
       " [81, 72, 38, 32, 196, 336, 131],\n",
       " [10, 270],\n",
       " [10, 270, 886],\n",
       " [10, 270, 886, 80],\n",
       " [10, 270, 886, 80, 10],\n",
       " [10, 270, 886, 80, 10, 1393],\n",
       " [128, 72],\n",
       " [128, 72, 1],\n",
       " [128, 72, 1, 1394],\n",
       " [128, 72, 1, 1394, 1395],\n",
       " [128, 72, 1, 1394, 1395, 84],\n",
       " [72, 1],\n",
       " [72, 1, 1396],\n",
       " [72, 1, 1396, 365],\n",
       " [72, 1, 1396, 365, 2],\n",
       " [72, 1, 1396, 365, 2, 887],\n",
       " [135, 7],\n",
       " [135, 7, 36],\n",
       " [135, 7, 36, 16],\n",
       " [135, 7, 36, 16, 172],\n",
       " [135, 7, 36, 16, 172, 1],\n",
       " [135, 7, 36, 16, 172, 1, 299],\n",
       " [135, 7, 36, 16, 172, 1, 299, 4],\n",
       " [135, 7, 36, 16, 172, 1, 299, 4, 666],\n",
       " [81, 76],\n",
       " [81, 76, 1],\n",
       " [81, 76, 1, 225],\n",
       " [128, 44],\n",
       " [128, 44, 1],\n",
       " [128, 44, 1, 888],\n",
       " [135, 73],\n",
       " [135, 73, 3],\n",
       " [135, 73, 3, 336],\n",
       " [135, 73, 3, 336, 14],\n",
       " [135, 73, 3, 336, 14, 41],\n",
       " [81, 5],\n",
       " [81, 5, 51],\n",
       " [81, 5, 51, 1397],\n",
       " [81, 5, 51, 1397, 1398],\n",
       " [27, 1399],\n",
       " [27, 1399, 889],\n",
       " [27, 1399, 889, 337],\n",
       " [27, 1399, 889, 337, 366],\n",
       " [27, 1399, 889, 337, 366, 11],\n",
       " [27, 1399, 889, 337, 366, 11, 420],\n",
       " [27, 1399, 889, 337, 366, 11, 420, 2],\n",
       " [27, 1399, 889, 337, 366, 11, 420, 2, 420],\n",
       " [27, 1399, 889, 337, 366, 11, 420, 2, 420, 11],\n",
       " [27, 1399, 889, 337, 366, 11, 420, 2, 420, 11, 366],\n",
       " [1400, 367],\n",
       " [1400, 367, 1],\n",
       " [1400, 367, 1, 1401],\n",
       " [1400, 367, 1, 1401, 2],\n",
       " [1400, 367, 1, 1401, 2, 890],\n",
       " [1400, 367, 1, 1401, 2, 890, 173],\n",
       " [129, 421],\n",
       " [422, 150],\n",
       " [422, 150, 33],\n",
       " [422, 150, 33, 52],\n",
       " [422, 150, 33, 52, 1402],\n",
       " [422, 150, 33, 52, 1402, 338],\n",
       " [422, 150, 33, 52, 1402, 338, 107],\n",
       " [422, 150, 33, 52, 1402, 338, 107, 14],\n",
       " [423, 6],\n",
       " [423, 6, 891],\n",
       " [423, 6, 891, 1403],\n",
       " [52, 25],\n",
       " [52, 25, 174],\n",
       " [52, 25, 174, 89],\n",
       " [52, 25, 174, 89, 11],\n",
       " [52, 25, 174, 89, 11, 7],\n",
       " [52, 25, 174, 89, 11, 7, 23],\n",
       " [52, 25, 174, 89, 11, 7, 23, 113],\n",
       " [52, 25, 174, 89, 11, 7, 23, 113, 271],\n",
       " [30, 1404],\n",
       " [30, 1404, 57],\n",
       " [30, 1404, 57, 15],\n",
       " [30, 1404, 57, 15, 1405],\n",
       " [30, 1404, 57, 15, 1405, 4],\n",
       " [30, 1404, 57, 15, 1405, 4, 1],\n",
       " [30, 1404, 57, 15, 1405, 4, 1, 892],\n",
       " [1, 667],\n",
       " [1, 667, 339],\n",
       " [114, 26],\n",
       " [114, 26, 11],\n",
       " [114, 26, 11, 1],\n",
       " [114, 26, 11, 1, 1406],\n",
       " [66, 67],\n",
       " [66, 67, 6],\n",
       " [66, 67, 6, 58],\n",
       " [66, 67, 6, 58, 2],\n",
       " [66, 67, 6, 58, 2, 1407],\n",
       " [66, 67, 6, 58, 2, 1407, 513],\n",
       " [66, 67, 6, 58, 2, 1407, 513, 893],\n",
       " [514, 8],\n",
       " [514, 8, 1408],\n",
       " [514, 8, 1408, 183],\n",
       " [514, 8, 1408, 183, 894],\n",
       " [514, 8, 1408, 183, 894, 424],\n",
       " [74, 3],\n",
       " [74, 3, 1],\n",
       " [74, 3, 1, 52],\n",
       " [74, 3, 1, 52, 1],\n",
       " [74, 3, 1, 52, 1, 668],\n",
       " [74, 3, 1, 52, 1, 668, 4],\n",
       " [74, 3, 1, 52, 1, 668, 4, 1],\n",
       " [74, 3, 1, 52, 1, 668, 4, 1, 1409],\n",
       " [30, 31],\n",
       " [30, 31, 669],\n",
       " [30, 31, 669, 184],\n",
       " [30, 31, 669, 184, 13],\n",
       " [670, 895],\n",
       " [670, 895, 13],\n",
       " [670, 895, 13, 515],\n",
       " [30, 226],\n",
       " [30, 226, 896],\n",
       " [30, 226, 896, 1410],\n",
       " [30, 226, 896, 1410, 7],\n",
       " [30, 226, 896, 1410, 7, 77],\n",
       " [30, 226, 896, 1410, 7, 77, 897],\n",
       " [30, 226, 896, 1410, 7, 77, 897, 425],\n",
       " [2, 1411],\n",
       " [2, 1411, 42],\n",
       " [2, 1411, 42, 144],\n",
       " [2, 1411, 42, 144, 1],\n",
       " [2, 1411, 42, 144, 1, 1412],\n",
       " [2, 1411, 42, 144, 1, 1412, 1413],\n",
       " [1414, 3],\n",
       " [1414, 3, 16],\n",
       " [1414, 3, 16, 6],\n",
       " [1414, 3, 16, 6, 898],\n",
       " [1414, 3, 16, 6, 898, 24],\n",
       " [1414, 3, 16, 6, 898, 24, 3],\n",
       " [1414, 3, 16, 6, 898, 24, 3, 7],\n",
       " [1, 1415],\n",
       " [1, 1415, 1416],\n",
       " [1, 1415, 1416, 4],\n",
       " [1, 1415, 1416, 4, 136],\n",
       " [77, 1417],\n",
       " [77, 1417, 44],\n",
       " [77, 1417, 44, 29],\n",
       " [77, 1417, 44, 29, 47],\n",
       " [77, 1417, 44, 29, 47, 1],\n",
       " [77, 1417, 44, 29, 47, 1, 1418],\n",
       " [77, 1417, 44, 29, 47, 1, 1418, 1419],\n",
       " [4, 671],\n",
       " [4, 671, 2],\n",
       " [4, 671, 2, 1420],\n",
       " [4, 671, 2, 1420, 11],\n",
       " [4, 671, 2, 1420, 11, 1421],\n",
       " [2, 340],\n",
       " [2, 340, 45],\n",
       " [2, 340, 45, 15],\n",
       " [2, 340, 45, 15, 672],\n",
       " [2, 340, 45, 15, 672, 899],\n",
       " [2, 340, 45, 15, 672, 899, 1422],\n",
       " [900, 67],\n",
       " [900, 67, 6],\n",
       " [900, 67, 6, 1423],\n",
       " [900, 67, 6, 1423, 1424],\n",
       " [900, 67, 6, 1423, 1424, 21],\n",
       " [900, 67, 6, 1423, 1424, 21, 516],\n",
       " [900, 67, 6, 1423, 1424, 21, 516, 103],\n",
       " [900, 67, 6, 1423, 1424, 21, 516, 103, 901],\n",
       " [24, 894],\n",
       " [24, 894, 41],\n",
       " [24, 894, 41, 95],\n",
       " [24, 894, 41, 95, 300],\n",
       " [24, 894, 41, 95, 300, 902],\n",
       " [24, 894, 41, 95, 300, 902, 7],\n",
       " [24, 894, 41, 95, 300, 902, 7, 185],\n",
       " [1425, 340],\n",
       " [1425, 340, 14],\n",
       " [1425, 340, 14, 15],\n",
       " [1425, 340, 14, 15, 1426],\n",
       " [1425, 340, 14, 15, 1426, 903],\n",
       " [34, 1427],\n",
       " [34, 1427, 14],\n",
       " [34, 1427, 14, 174],\n",
       " [34, 1427, 14, 174, 673],\n",
       " [67, 1428],\n",
       " [67, 1428, 1429],\n",
       " [67, 1428, 1429, 1430],\n",
       " [67, 1428, 1429, 1430, 109],\n",
       " [67, 1428, 1429, 1430, 109, 15],\n",
       " [67, 1428, 1429, 1430, 109, 15, 904],\n",
       " [137, 300],\n",
       " [137, 300, 674],\n",
       " [137, 300, 674, 1],\n",
       " [137, 300, 674, 1, 675],\n",
       " [34, 1431],\n",
       " [34, 1431, 1432],\n",
       " [34, 1431, 1432, 214],\n",
       " [34, 1431, 1432, 214, 116],\n",
       " [34, 1431, 1432, 214, 116, 426],\n",
       " [34, 1431, 1432, 214, 116, 426, 905],\n",
       " [34, 1431, 1432, 214, 116, 426, 905, 3],\n",
       " [34, 1431, 1432, 214, 116, 426, 905, 3, 29],\n",
       " [137, 23],\n",
       " [137, 23, 1433],\n",
       " [137, 23, 1433, 29],\n",
       " [137, 23, 1433, 29, 47],\n",
       " [137, 23, 1433, 29, 47, 1],\n",
       " [137, 23, 1433, 29, 47, 1, 1434],\n",
       " [137, 23, 1433, 29, 47, 1, 1434, 427],\n",
       " [137, 23, 1433, 29, 47, 1, 1434, 427, 1435],\n",
       " [2, 1436],\n",
       " [2, 1436, 15],\n",
       " [2, 1436, 15, 272],\n",
       " [2, 1436, 15, 272, 44],\n",
       " [2, 1436, 15, 272, 44, 19],\n",
       " [2, 1436, 15, 272, 44, 19, 906],\n",
       " [52, 151],\n",
       " [52, 151, 517],\n",
       " [52, 151, 517, 907],\n",
       " [52, 151, 517, 907, 162],\n",
       " [52, 151, 517, 907, 162, 676],\n",
       " [670, 30],\n",
       " [670, 30, 369],\n",
       " [670, 30, 369, 1],\n",
       " [670, 30, 369, 1, 666],\n",
       " [670, 30, 369, 1, 666, 1437],\n",
       " [670, 30, 369, 1, 666, 1437, 15],\n",
       " [670, 30, 369, 1, 666, 1437, 15, 1438],\n",
       " [1439, 1440],\n",
       " [1439, 1440, 2],\n",
       " [1439, 1440, 2, 1441],\n",
       " [1439, 1440, 2, 1441, 1442],\n",
       " [28, 47],\n",
       " [28, 47, 7],\n",
       " [28, 47, 7, 908],\n",
       " [28, 47, 7, 908, 369],\n",
       " [28, 47, 7, 908, 369, 428],\n",
       " [28, 47, 7, 908, 369, 428, 677],\n",
       " [28, 47, 7, 908, 369, 428, 677, 3],\n",
       " [28, 47, 7, 908, 369, 428, 677, 3, 51],\n",
       " [909, 1443],\n",
       " [909, 1443, 678],\n",
       " [909, 1443, 678, 52],\n",
       " [909, 1443, 678, 52, 4],\n",
       " [909, 1443, 678, 52, 4, 215],\n",
       " [909, 1443, 678, 52, 4, 215, 678],\n",
       " [39, 910],\n",
       " [39, 910, 679],\n",
       " [39, 910, 679, 90],\n",
       " [39, 910, 679, 90, 14],\n",
       " [39, 910, 679, 90, 14, 518],\n",
       " [39, 910, 679, 90, 14, 518, 911],\n",
       " [1444, 123],\n",
       " [1444, 123, 1445],\n",
       " [1444, 123, 1445, 671],\n",
       " [1444, 123, 1445, 671, 3],\n",
       " [1444, 123, 1445, 671, 3, 519],\n",
       " [1444, 123, 1445, 671, 3, 519, 42],\n",
       " [1444, 123, 1445, 671, 3, 519, 42, 912],\n",
       " [21, 1],\n",
       " [21, 1, 680],\n",
       " [21, 1, 680, 78],\n",
       " [21, 1, 680, 78, 1446],\n",
       " [21, 1, 680, 78, 1446, 681],\n",
       " [14, 1447],\n",
       " [14, 1447, 913],\n",
       " [14, 1447, 913, 2],\n",
       " [14, 1447, 913, 2, 197],\n",
       " [14, 1447, 913, 2, 197, 1448],\n",
       " [14, 1447, 913, 2, 197, 1448, 4],\n",
       " [14, 1447, 913, 2, 197, 1448, 4, 152],\n",
       " [682, 6],\n",
       " [682, 6, 1449],\n",
       " [682, 6, 1449, 1450],\n",
       " [52, 1451],\n",
       " [52, 1451, 12],\n",
       " [52, 1451, 12, 26],\n",
       " [52, 1451, 12, 26, 19],\n",
       " [52, 1451, 12, 26, 19, 1452],\n",
       " [52, 1451, 12, 26, 19, 1452, 41],\n",
       " [52, 1451, 12, 26, 19, 1452, 41, 2],\n",
       " [670, 914],\n",
       " [670, 914, 30],\n",
       " [670, 914, 30, 1454],\n",
       " [670, 914, 30, 1454, 1455],\n",
       " [80, 1],\n",
       " [80, 1, 1456],\n",
       " [80, 1, 1456, 1],\n",
       " [80, 1, 1456, 1, 915],\n",
       " [69, 5],\n",
       " [69, 5, 74],\n",
       " [69, 5, 74, 916],\n",
       " [69, 5, 74, 916, 5],\n",
       " [69, 5, 74, 916, 5, 82],\n",
       " [69, 5, 74, 916, 5, 82, 271],\n",
       " [69, 5, 74, 916, 5, 82, 271, 37],\n",
       " [69, 5, 74, 916, 5, 82, 271, 37, 75],\n",
       " [30, 1457],\n",
       " [30, 1457, 429],\n",
       " [30, 1457, 429, 683],\n",
       " [30, 1457, 429, 683, 14],\n",
       " [30, 1457, 429, 683, 14, 227],\n",
       " [30, 1457, 429, 683, 14, 227, 1458],\n",
       " [28, 37],\n",
       " [28, 37, 1459],\n",
       " [28, 37, 1459, 1460],\n",
       " [28, 37, 1459, 1460, 917],\n",
       " [28, 37, 1459, 1460, 917, 44],\n",
       " [28, 37, 1459, 1460, 917, 44, 1],\n",
       " [28, 37, 1459, 1460, 917, 44, 1, 1461],\n",
       " [1462, 37],\n",
       " [1462, 37, 1463],\n",
       " [1462, 37, 1463, 3],\n",
       " [1462, 37, 1463, 3, 1464],\n",
       " [1462, 37, 1463, 3, 1464, 10],\n",
       " [1462, 37, 1463, 3, 1464, 10, 1465],\n",
       " [1462, 37, 1463, 3, 1464, 10, 1465, 684],\n",
       " [80, 1466],\n",
       " [80, 1466, 370],\n",
       " [80, 1466, 370, 1467],\n",
       " [5, 138],\n",
       " [5, 138, 254],\n",
       " [5, 138, 254, 21],\n",
       " [5, 138, 254, 21, 5],\n",
       " [5, 138, 254, 21, 5, 104],\n",
       " [5, 138, 254, 21, 5, 104, 1468],\n",
       " [8, 685],\n",
       " [8, 685, 273],\n",
       " [8, 685, 273, 24],\n",
       " [8, 685, 273, 24, 371],\n",
       " [52, 28],\n",
       " [52, 28, 95],\n",
       " [52, 28, 95, 49],\n",
       " [52, 28, 95, 49, 255],\n",
       " [52, 28, 95, 49, 255, 520],\n",
       " [52, 28, 95, 49, 255, 520, 43],\n",
       " [52, 28, 95, 49, 255, 520, 43, 30],\n",
       " [52, 28, 95, 49, 255, 520, 43, 30, 49],\n",
       " [52, 28, 95, 49, 255, 520, 43, 30, 49, 684],\n",
       " [37, 1469],\n",
       " [37, 1469, 4],\n",
       " [37, 1469, 4, 228],\n",
       " [37, 1469, 4, 228, 145],\n",
       " [37, 1469, 4, 228, 145, 175],\n",
       " [37, 1469, 4, 228, 145, 175, 274],\n",
       " [37, 1469, 4, 228, 145, 175, 274, 29],\n",
       " [37, 1469, 4, 228, 145, 175, 274, 29, 1470],\n",
       " [33, 59],\n",
       " [33, 59, 2],\n",
       " [33, 59, 2, 372],\n",
       " [66, 198],\n",
       " [66, 198, 153],\n",
       " [114, 1],\n",
       " [114, 1, 162],\n",
       " [114, 1, 162, 115],\n",
       " [114, 1, 162, 115, 4],\n",
       " [114, 1, 162, 115, 4, 59],\n",
       " [107, 25],\n",
       " [107, 25, 6],\n",
       " [107, 25, 6, 1471],\n",
       " [107, 25, 6, 1471, 918],\n",
       " [107, 25, 6, 1471, 918, 367],\n",
       " [107, 25, 6, 1471, 918, 367, 15],\n",
       " [107, 25, 6, 1471, 918, 367, 15, 229],\n",
       " [28, 70],\n",
       " [28, 70, 23],\n",
       " [28, 70, 23, 117],\n",
       " [28, 70, 23, 117, 7],\n",
       " [28, 70, 23, 117, 7, 430],\n",
       " [28, 70, 23, 117, 7, 430, 3],\n",
       " [28, 70, 23, 117, 7, 430, 3, 105],\n",
       " [28, 70, 23, 117, 7, 430, 3, 105, 124],\n",
       " [28, 70, 23, 117, 7, 430, 3, 105, 124, 163],\n",
       " [59, 186],\n",
       " [59, 186, 1472],\n",
       " [59, 186, 1472, 1],\n",
       " [59, 186, 1472, 1, 52],\n",
       " [52, 369],\n",
       " [52, 369, 1473],\n",
       " [52, 369, 1473, 31],\n",
       " [52, 369, 1473, 31, 162],\n",
       " [52, 369, 1473, 31, 162, 115],\n",
       " [59, 47],\n",
       " [59, 47, 1474],\n",
       " [59, 47, 1474, 99],\n",
       " [59, 47, 1474, 99, 52],\n",
       " [76, 1],\n",
       " [76, 1, 680],\n",
       " [76, 1, 680, 919],\n",
       " [76, 1, 680, 919, 1475],\n",
       " [76, 1, 680, 919, 1475, 1],\n",
       " [76, 1, 680, 919, 1475, 1, 1476],\n",
       " [2, 1477],\n",
       " [2, 1477, 19],\n",
       " [2, 1477, 19, 521],\n",
       " [2, 1477, 19, 521, 373],\n",
       " [920, 301],\n",
       " [920, 301, 14],\n",
       " [920, 301, 14, 431],\n",
       " [920, 301, 14, 431, 921],\n",
       " [1478, 57],\n",
       " [1478, 57, 7],\n",
       " [1478, 57, 7, 125],\n",
       " [1478, 57, 7, 125, 1479],\n",
       " [1478, 57, 7, 125, 1479, 1480],\n",
       " [1, 115],\n",
       " [1, 115, 4],\n",
       " [1, 115, 4, 132],\n",
       " [1, 115, 4, 132, 682],\n",
       " [1, 115, 4, 132, 682, 6],\n",
       " [1, 115, 4, 132, 682, 6, 686],\n",
       " [1, 115, 4, 132, 682, 6, 686, 1481],\n",
       " [137, 7],\n",
       " [137, 7, 1482],\n",
       " [137, 7, 1482, 1483],\n",
       " [137, 7, 1482, 1483, 1484],\n",
       " [137, 7, 1482, 1483, 1484, 10],\n",
       " [137, 7, 1482, 1483, 1484, 10, 1485],\n",
       " [1486, 29],\n",
       " [1486, 29, 14],\n",
       " [1486, 29, 14, 85],\n",
       " [1486, 29, 14, 85, 1487],\n",
       " [432, 187],\n",
       " [432, 187, 432],\n",
       " [432, 187, 432, 922],\n",
       " [432, 187, 432, 922, 522],\n",
       " [432, 187, 432, 922, 522, 514],\n",
       " [432, 187, 432, 922, 522, 514, 522],\n",
       " [1488, 15],\n",
       " [1488, 15, 1489],\n",
       " [1488, 15, 1489, 687],\n",
       " [1488, 15, 1489, 687, 2],\n",
       " [1488, 15, 1489, 687, 2, 3],\n",
       " [1488, 15, 1489, 687, 2, 3, 1490],\n",
       " [1, 1491],\n",
       " [1, 1491, 302],\n",
       " [1, 1491, 302, 45],\n",
       " [1, 1491, 302, 45, 50],\n",
       " [52, 99],\n",
       " [52, 99, 1492],\n",
       " [59, 7],\n",
       " [59, 7, 53],\n",
       " [59, 7, 53, 1493],\n",
       " [59, 7, 53, 1493, 1],\n",
       " [59, 7, 53, 1493, 1, 1494],\n",
       " [59, 7, 53, 1493, 1, 1494, 52],\n",
       " [1495, 1496],\n",
       " [116, 62],\n",
       " [116, 62, 32],\n",
       " [116, 62, 32, 1497],\n",
       " [116, 62, 32, 1497, 29],\n",
       " [116, 62, 32, 1497, 29, 1498],\n",
       " [116, 62, 32, 1497, 29, 1498, 4],\n",
       " [116, 62, 32, 1497, 29, 1498, 4, 15],\n",
       " [116, 62, 32, 1497, 29, 1498, 4, 15, 152],\n",
       " [137, 23],\n",
       " [137, 23, 1499],\n",
       " [137, 23, 1499, 54],\n",
       " [137, 23, 1499, 54, 1500],\n",
       " [137, 23, 1499, 54, 1500, 1501],\n",
       " [137, 23, 1499, 54, 1500, 1501, 1502],\n",
       " [433, 523],\n",
       " [433, 523, 1503],\n",
       " [433, 523, 1503, 3],\n",
       " [433, 523, 1503, 3, 19],\n",
       " [433, 523, 1503, 3, 19, 524],\n",
       " [433, 523, 1503, 3, 19, 524, 374],\n",
       " [52, 39],\n",
       " [52, 39, 56],\n",
       " [52, 39, 56, 7],\n",
       " [52, 39, 56, 7, 115],\n",
       " [52, 39, 56, 7, 115, 4],\n",
       " [52, 39, 56, 7, 115, 4, 132],\n",
       " [52, 39, 56, 7, 115, 4, 132, 38],\n",
       " [52, 39, 56, 7, 115, 4, 132, 38, 1504],\n",
       " [19, 525],\n",
       " [19, 525, 1505],\n",
       " [19, 525, 1505, 175],\n",
       " [19, 525, 1505, 175, 526],\n",
       " [19, 525, 1505, 175, 526, 15],\n",
       " [19, 525, 1505, 175, 526, 15, 303],\n",
       " [19, 525, 1505, 175, 526, 15, 303, 139],\n",
       " [2, 14],\n",
       " [2, 14, 15],\n",
       " [2, 14, 15, 434],\n",
       " [2, 14, 15, 434, 435],\n",
       " [2, 14, 15, 434, 435, 923],\n",
       " [2, 14, 15, 434, 435, 923, 41],\n",
       " [59, 86],\n",
       " [59, 86, 100],\n",
       " [59, 86, 100, 13],\n",
       " [59, 86, 100, 13, 84],\n",
       " [52, 25],\n",
       " [52, 25, 23],\n",
       " [52, 25, 23, 55],\n",
       " [52, 25, 23, 55, 365],\n",
       " [52, 25, 23, 55, 365, 199],\n",
       " [52, 25, 23, 55, 365, 199, 41],\n",
       " [52, 25, 23, 55, 365, 199, 41, 55],\n",
       " [52, 25, 23, 55, 365, 199, 41, 55, 887],\n",
       " [129, 436],\n",
       " [270, 33],\n",
       " [270, 33, 1],\n",
       " [270, 33, 1, 196],\n",
       " [270, 33, 1, 196, 298],\n",
       " [81, 76],\n",
       " [81, 76, 304],\n",
       " [81, 76, 304, 31],\n",
       " [81, 76, 304, 31, 230],\n",
       " [81, 76, 304, 31, 230, 924],\n",
       " [128, 1506],\n",
       " [128, 1506, 1507],\n",
       " [135, 924],\n",
       " [135, 924, 76],\n",
       " [135, 924, 76, 31],\n",
       " [81, 6],\n",
       " [81, 6, 1508],\n",
       " [81, 6, 1508, 91],\n",
       " [81, 6, 1508, 91, 90],\n",
       " [81, 6, 1508, 91, 90, 1509],\n",
       " [81, 6, 1508, 91, 90, 1509, 10],\n",
       " [81, 6, 1508, 91, 90, 1509, 10, 68],\n",
       " [81, 6, 1508, 91, 90, 1509, 10, 68, 1510],\n",
       " [2, 688],\n",
       " [2, 688, 688],\n",
       " [2, 688, 688, 2],\n",
       " [2, 688, 688, 2, 688],\n",
       " [118, 22],\n",
       " [118, 22, 1511],\n",
       " [118, 22, 1511, 5],\n",
       " [1512, 43],\n",
       " [1512, 43, 1513],\n",
       " [1512, 43, 1513, 1],\n",
       " [1512, 43, 1513, 1, 1514],\n",
       " [1512, 43, 1513, 1, 1514, 925],\n",
       " [1512, 43, 1513, 1, 1514, 925, 1515],\n",
       " [1512, 43, 1513, 1, 1514, 925, 1515, 926],\n",
       " [68, 1516],\n",
       " [68, 1516, 3],\n",
       " [68, 1516, 3, 1517],\n",
       " [68, 1516, 3, 1517, 305],\n",
       " [68, 1516, 3, 1517, 305, 375],\n",
       " [68, 1516, 3, 1517, 305, 375, 200],\n",
       " [68, 1516, 3, 1517, 305, 375, 200, 927],\n",
       " [21, 10],\n",
       " [21, 10, 6],\n",
       " [21, 10, 6, 1518],\n",
       " [21, 10, 6, 1518, 86],\n",
       " [21, 10, 6, 1518, 86, 437],\n",
       " [21, 10, 6, 1518, 86, 437, 1519],\n",
       " [2, 67],\n",
       " [2, 67, 6],\n",
       " [2, 67, 6, 1520],\n",
       " [2, 67, 6, 1520, 188],\n",
       " [2, 67, 6, 1520, 188, 6],\n",
       " [2, 67, 6, 1520, 188, 6, 1521],\n",
       " [86, 77],\n",
       " [86, 77, 86],\n",
       " [86, 77, 86, 77],\n",
       " [86, 77, 86, 77, 2],\n",
       " [86, 77, 86, 77, 2, 86],\n",
       " [86, 77, 86, 77, 2, 86, 77],\n",
       " [128, 86],\n",
       " [128, 86, 118],\n",
       " [128, 86, 118, 43],\n",
       " [128, 86, 118, 43, 6],\n",
       " [128, 86, 118, 43, 6, 527],\n",
       " [81, 1522],\n",
       " [81, 1522, 689],\n",
       " [135, 2],\n",
       " [135, 2, 5],\n",
       " [135, 2, 5, 370],\n",
       " [81, 5],\n",
       " [81, 5, 8],\n",
       " [81, 5, 8, 85],\n",
       " [81, 5, 8, 85, 20],\n",
       " [81, 5, 8, 85, 20, 27],\n",
       " [81, 5, 8, 85, 20, 27, 1],\n",
       " [81, 5, 8, 85, 20, 27, 1, 176],\n",
       " [2, 1],\n",
       " [2, 1, 231],\n",
       " [2, 1, 231, 1523],\n",
       " [2, 1, 231, 1523, 37],\n",
       " [2, 1, 231, 1523, 37, 528],\n",
       " [27, 1],\n",
       " [27, 1, 1524],\n",
       " [27, 1, 1524, 7],\n",
       " [27, 1, 1524, 7, 37],\n",
       " [27, 1, 1524, 7, 37, 96],\n",
       " [177, 1525],\n",
       " [177, 1525, 529],\n",
       " [177, 1525, 529, 1526],\n",
       " [86, 1527],\n",
       " [86, 1527, 29],\n",
       " [86, 1527, 29, 1528],\n",
       " [86, 1527, 29, 1528, 30],\n",
       " [86, 1527, 29, 1528, 30, 1529],\n",
       " [126, 38],\n",
       " [126, 38, 928],\n",
       " [126, 38, 928, 92],\n",
       " [126, 38, 928, 92, 116],\n",
       " [126, 38, 928, 92, 116, 141],\n",
       " [306, 44],\n",
       " [306, 44, 15],\n",
       " [306, 44, 15, 1530],\n",
       " [306, 44, 15, 1530, 275],\n",
       " [306, 44, 15, 1530, 275, 1531],\n",
       " [23, 38],\n",
       " [23, 38, 232],\n",
       " [23, 38, 232, 6],\n",
       " [23, 38, 232, 6, 89],\n",
       " [23, 38, 232, 6, 89, 929],\n",
       " [930, 1532],\n",
       " [930, 1532, 530],\n",
       " [930, 1532, 530, 531],\n",
       " [930, 1532, 530, 531, 530],\n",
       " [38, 23],\n",
       " [38, 23, 1533],\n",
       " [38, 23, 1533, 1534],\n",
       " [38, 23, 1533, 1534, 2],\n",
       " [38, 23, 1533, 1534, 2, 931],\n",
       " [154, 15],\n",
       " [154, 15, 1535],\n",
       " [154, 15, 1535, 138],\n",
       " [154, 15, 1535, 138, 16],\n",
       " [154, 15, 1535, 138, 16, 365],\n",
       " [48, 13],\n",
       " [48, 13, 38],\n",
       " [48, 13, 38, 16],\n",
       " [48, 13, 38, 16, 1536],\n",
       " [48, 13, 38, 16, 1536, 1537],\n",
       " [117, 25],\n",
       " [117, 25, 5],\n",
       " [117, 25, 5, 20],\n",
       " [128, 164],\n",
       " [128, 164, 22],\n",
       " [128, 164, 22, 164],\n",
       " [128, 164, 22, 164, 22],\n",
       " [81, 153],\n",
       " [81, 153, 5],\n",
       " [81, 153, 5, 20],\n",
       " [81, 153, 5, 20, 6],\n",
       " [81, 153, 5, 20, 6, 1538],\n",
       " [81, 153, 5, 20, 6, 1538, 1539],\n",
       " [1540, 30],\n",
       " [1540, 30, 1541],\n",
       " [1540, 30, 1541, 23],\n",
       " [1540, 30, 1541, 23, 71],\n",
       " [1540, 30, 1541, 23, 71, 51],\n",
       " [532, 150],\n",
       " [135, 6],\n",
       " [135, 6, 533],\n",
       " [135, 6, 533, 6],\n",
       " [135, 6, 533, 6, 533],\n",
       " [41, 376],\n",
       " [41, 376, 51],\n",
       " [27, 1],\n",
       " [27, 1, 690],\n",
       " [27, 1, 690, 341],\n",
       " [27, 1, 690, 341, 127],\n",
       " [27, 1, 690, 341, 127, 10],\n",
       " [27, 1, 690, 341, 127, 10, 127],\n",
       " [1542, 4],\n",
       " [1542, 4, 1],\n",
       " [1542, 4, 1, 691],\n",
       " [1542, 4, 1, 691, 2],\n",
       " [1542, 4, 1, 691, 2, 692],\n",
       " [201, 77],\n",
       " [201, 77, 175],\n",
       " [201, 77, 175, 202],\n",
       " [201, 77, 175, 202, 202],\n",
       " [438, 3],\n",
       " [438, 3, 256],\n",
       " [438, 3, 256, 2],\n",
       " [438, 3, 256, 2, 438],\n",
       " [438, 3, 256, 2, 438, 3],\n",
       " [438, 3, 256, 2, 438, 3, 110],\n",
       " [2, 438],\n",
       " [2, 438, 131],\n",
       " [2, 438, 131, 3],\n",
       " [2, 438, 131, 3, 79],\n",
       " [2, 438, 131, 3, 79, 111],\n",
       " [2, 438, 131, 3, 79, 111, 530],\n",
       " [233, 1],\n",
       " [233, 1, 1543],\n",
       " [233, 1, 1543, 932],\n",
       " [233, 1, 1543, 932, 111],\n",
       " [33, 41],\n",
       " [33, 41, 2],\n",
       " [33, 41, 2, 93],\n",
       " [17, 28],\n",
       " [17, 28, 420],\n",
       " [17, 28, 420, 2],\n",
       " [17, 28, 420, 2, 366],\n",
       " [17, 28, 420, 2, 366, 6],\n",
       " [17, 28, 420, 2, 366, 6, 141],\n",
       " [17, 28, 420, 2, 366, 6, 141, 5],\n",
       " [17, 28, 420, 2, 366, 6, 141, 5, 20],\n",
       " [17, 28, 420, 2, 366, 6, 141, 5, 20, 12],\n",
       " [17, 28, 420, 2, 366, 6, 141, 5, 20, 12, 377],\n",
       " [93, 94],\n",
       " [93, 94, 439],\n",
       " [93, 94, 439, 307],\n",
       " [93, 94, 439, 307, 440],\n",
       " [93, 94, 439, 307, 440, 3],\n",
       " [93, 94, 439, 307, 440, 3, 1544],\n",
       " [93, 94, 439, 307, 440, 3, 1544, 25],\n",
       " [93, 94, 439, 307, 440, 3, 1544, 25, 35],\n",
       " [93, 94, 439, 307, 440, 3, 1544, 25, 35, 123],\n",
       " [28, 933],\n",
       " [28, 933, 2],\n",
       " [28, 933, 2, 28],\n",
       " [28, 933, 2, 28, 693],\n",
       " [28, 933, 2, 28, 693, 10],\n",
       " [28, 933, 2, 28, 693, 10, 42],\n",
       " [28, 933, 2, 28, 693, 10, 42, 1545],\n",
       " [7, 117],\n",
       " [7, 117, 12],\n",
       " [7, 117, 12, 67],\n",
       " [7, 117, 12, 67, 112],\n",
       " [7, 117, 12, 67, 112, 1546],\n",
       " [7, 117, 12, 67, 112, 1546, 200],\n",
       " [7, 117, 12, 67, 112, 1546, 200, 308],\n",
       " [2, 48],\n",
       " [2, 48, 35],\n",
       " [2, 48, 35, 534],\n",
       " [2, 48, 35, 534, 232],\n",
       " [2, 48, 35, 534, 232, 9],\n",
       " [2, 48, 35, 534, 232, 9, 80],\n",
       " [2, 48, 35, 534, 232, 9, 80, 35],\n",
       " [2, 48, 35, 534, 232, 9, 80, 35, 9],\n",
       " [2, 48, 35, 534, 232, 9, 80, 35, 9, 1547],\n",
       " [7, 89],\n",
       " [7, 89, 101],\n",
       " [7, 89, 101, 535],\n",
       " [7, 89, 101, 535, 9],\n",
       " [7, 89, 101, 535, 9, 342],\n",
       " [7, 89, 101, 535, 9, 342, 3],\n",
       " [7, 89, 101, 535, 9, 342, 3, 1548],\n",
       " [7, 89, 101, 535, 9, 342, 3, 1548, 22],\n",
       " [57, 203],\n",
       " [57, 203, 54],\n",
       " [57, 203, 54, 234],\n",
       " [57, 203, 54, 234, 68],\n",
       " [57, 203, 54, 234, 68, 1549],\n",
       " [57, 203, 54, 234, 68, 1549, 934],\n",
       " [57, 203, 54, 234, 68, 1549, 934, 1550],\n",
       " [44, 68],\n",
       " [44, 68, 1551],\n",
       " [44, 68, 1551, 694],\n",
       " [44, 68, 1551, 694, 9],\n",
       " [44, 68, 1551, 694, 9, 70],\n",
       " [44, 68, 1551, 694, 9, 70, 16],\n",
       " [44, 68, 1551, 694, 9, 70, 16, 441],\n",
       " [2, 48],\n",
       " [2, 48, 18],\n",
       " [2, 48, 18, 1552],\n",
       " [2, 48, 18, 1552, 929],\n",
       " [2, 48, 18, 1552, 929, 22],\n",
       " [2, 48, 18, 1552, 929, 22, 3],\n",
       " [2, 48, 18, 1552, 929, 22, 3, 1553],\n",
       " [7, 9],\n",
       " [7, 9, 35],\n",
       " [7, 9, 35, 28],\n",
       " [257, 105],\n",
       " [257, 105, 69],\n",
       " [257, 105, 69, 9],\n",
       " [257, 105, 69, 9, 113],\n",
       " [257, 105, 69, 9, 113, 25],\n",
       " [257, 105, 69, 9, 113, 25, 35],\n",
       " [257, 105, 69, 9, 113, 25, 35, 9],\n",
       " [81, 27],\n",
       " [81, 27, 183],\n",
       " [81, 27, 183, 41],\n",
       " [81, 27, 183, 41, 183],\n",
       " [81, 27, 183, 41, 183, 3],\n",
       " [81, 27, 183, 41, 183, 3, 43],\n",
       " [81, 27, 183, 41, 183, 3, 43, 115],\n",
       " [81, 27, 183, 41, 183, 3, 43, 115, 4],\n",
       " [81, 27, 183, 41, 183, 3, 43, 115, 4, 536],\n",
       " [128, 27],\n",
       " [128, 27, 183],\n",
       " [128, 27, 183, 41],\n",
       " [128, 27, 183, 41, 183],\n",
       " [128, 27, 183, 41, 183, 3],\n",
       " [128, 27, 183, 41, 183, 3, 43],\n",
       " [128, 27, 183, 41, 183, 3, 43, 115],\n",
       " [128, 27, 183, 41, 183, 3, 43, 115, 4],\n",
       " [128, 27, 183, 41, 183, 3, 43, 115, 4, 132],\n",
       " [135, 27],\n",
       " [135, 27, 183],\n",
       " [135, 27, 183, 41],\n",
       " [135, 27, 183, 41, 7],\n",
       " [135, 27, 183, 41, 7, 378],\n",
       " [135, 27, 183, 41, 7, 378, 16],\n",
       " [135, 27, 183, 41, 7, 378, 16, 52],\n",
       " [135, 27, 183, 41, 7, 378, 16, 52, 695],\n",
       " [119, 58],\n",
       " [119, 58, 106],\n",
       " [119, 58, 106, 97],\n",
       " [119, 58, 106, 97, 77],\n",
       " [119, 58, 106, 97, 77, 9],\n",
       " [119, 58, 106, 97, 77, 9, 537],\n",
       " [119, 58, 106, 97, 77, 9, 537, 2],\n",
       " [119, 58, 106, 97, 77, 9, 537, 2, 342],\n",
       " [119, 58, 106, 97, 77, 9, 537, 2, 342, 3],\n",
       " [119, 58, 106, 97, 77, 9, 537, 2, 342, 3, 87],\n",
       " [124, 7],\n",
       " [124, 7, 77],\n",
       " [124, 7, 77, 442],\n",
       " [124, 7, 77, 442, 28],\n",
       " [124, 7, 77, 442, 28, 366],\n",
       " [124, 7, 77, 442, 28, 366, 177],\n",
       " [124, 7, 77, 442, 28, 366, 177, 185],\n",
       " [124, 7, 77, 442, 28, 366, 177, 185, 4],\n",
       " [124, 7, 77, 442, 28, 366, 177, 185, 4, 309],\n",
       " [35, 696],\n",
       " [35, 696, 935],\n",
       " [35, 696, 935, 80],\n",
       " [35, 696, 935, 80, 7],\n",
       " [35, 696, 935, 80, 7, 443],\n",
       " [34, 1554],\n",
       " [34, 1554, 696],\n",
       " [34, 1554, 696, 164],\n",
       " [34, 1554, 696, 164, 8],\n",
       " [34, 1554, 696, 164, 8, 199],\n",
       " [34, 1554, 696, 164, 8, 199, 936],\n",
       " [9, 923],\n",
       " [9, 923, 14],\n",
       " [9, 923, 14, 303],\n",
       " [9, 923, 14, 303, 258],\n",
       " [9, 923, 14, 303, 258, 2],\n",
       " [9, 923, 14, 303, 258, 2, 99],\n",
       " [9, 923, 14, 303, 258, 2, 99, 1555],\n",
       " [4, 199],\n",
       " [4, 199, 538],\n",
       " [4, 199, 538, 2],\n",
       " [4, 199, 538, 2, 4],\n",
       " [4, 199, 538, 2, 4, 276],\n",
       " [4, 199, 538, 2, 4, 276, 277],\n",
       " [7, 23],\n",
       " [7, 23, 430],\n",
       " [7, 23, 430, 1556],\n",
       " [7, 23, 430, 1556, 379],\n",
       " [7, 23, 430, 1556, 379, 3],\n",
       " [7, 23, 430, 1556, 379, 3, 22],\n",
       " [7, 23, 430, 1556, 379, 3, 22, 9],\n",
       " [7, 23, 430, 1556, 379, 3, 22, 9, 105],\n",
       " [7, 23, 430, 1556, 379, 3, 22, 9, 105, 12],\n",
       " [69, 9],\n",
       " [69, 9, 113],\n",
       " [69, 9, 113, 117],\n",
       " [69, 9, 113, 117, 216],\n",
       " [69, 9, 113, 117, 216, 1],\n",
       " [69, 9, 113, 117, 216, 1, 937],\n",
       " [69, 9, 113, 117, 216, 1, 937, 4],\n",
       " [69, 9, 113, 117, 216, 1, 937, 4, 63],\n",
       " [2, 74],\n",
       " [2, 74, 34],\n",
       " [2, 74, 34, 1557],\n",
       " [2, 74, 34, 1557, 36],\n",
       " [2, 74, 34, 1557, 36, 938],\n",
       " [2, 74, 34, 1557, 36, 938, 2],\n",
       " [2, 74, 34, 1557, 36, 938, 2, 34],\n",
       " [2, 74, 34, 1557, 36, 938, 2, 34, 36],\n",
       " [2, 74, 34, 1557, 36, 938, 2, 34, 36, 12],\n",
       " [105, 40],\n",
       " [105, 40, 3],\n",
       " [105, 40, 3, 22],\n",
       " [105, 40, 3, 22, 66],\n",
       " [105, 40, 3, 22, 66, 928],\n",
       " [105, 40, 3, 22, 66, 928, 939],\n",
       " [105, 40, 3, 22, 66, 928, 939, 116],\n",
       " [105, 40, 3, 22, 66, 928, 939, 116, 87],\n",
       " [18, 1558],\n",
       " [18, 1558, 116],\n",
       " [18, 1558, 116, 18],\n",
       " [18, 1558, 116, 18, 940],\n",
       " [81, 697],\n",
       " [128, 697],\n",
       " [135, 697],\n",
       " [81, 941],\n",
       " [81, 941, 1559],\n",
       " [81, 941, 1559, 41],\n",
       " [81, 941, 1559, 41, 2],\n",
       " [81, 941, 1559, 41, 2, 698],\n",
       " [128, 12],\n",
       " [128, 12, 28],\n",
       " [128, 12, 28, 699],\n",
       " [128, 12, 28, 699, 48],\n",
       " [128, 12, 28, 699, 48, 204],\n",
       " [128, 12, 28, 699, 48, 204, 1560],\n",
       " [135, 31],\n",
       " [135, 31, 378],\n",
       " [135, 31, 378, 274],\n",
       " [135, 31, 378, 274, 235],\n",
       " [135, 31, 378, 274, 235, 154],\n",
       " [135, 31, 378, 274, 235, 154, 31],\n",
       " [135, 31, 378, 274, 235, 154, 31, 16],\n",
       " [135, 31, 378, 274, 235, 154, 31, 16, 205],\n",
       " [28, 27],\n",
       " [28, 27, 183],\n",
       " [28, 27, 183, 41],\n",
       " [28, 27, 183, 41, 2],\n",
       " [28, 27, 183, 41, 2, 93],\n",
       " [81, 93],\n",
       " [81, 93, 2],\n",
       " [81, 93, 2, 41],\n",
       " [81, 93, 2, 41, 27],\n",
       " [81, 93, 2, 41, 27, 183],\n",
       " [17, 444],\n",
       " [17, 444, 9],\n",
       " [17, 444, 9, 1561],\n",
       " [17, 444, 9, 1561, 1562],\n",
       " [17, 444, 9, 1561, 1562, 254],\n",
       " [17, 444, 9, 1561, 1562, 254, 22],\n",
       " [17, 444, 9, 1561, 1562, 254, 22, 56],\n",
       " [57, 1563],\n",
       " [57, 1563, 139],\n",
       " [57, 1563, 139, 5],\n",
       " [57, 1563, 139, 5, 96],\n",
       " [57, 1563, 139, 5, 96, 5],\n",
       " [57, 1563, 139, 5, 96, 5, 104],\n",
       " [57, 1563, 139, 5, 96, 5, 104, 115],\n",
       " [57, 1563, 139, 5, 96, 5, 104, 115, 4],\n",
       " [57, 1563, 139, 5, 96, 5, 104, 115, 4, 536],\n",
       " [21, 94],\n",
       " [21, 94, 4],\n",
       " [21, 94, 4, 132],\n",
       " [21, 94, 4, 132, 1],\n",
       " [21, 94, 4, 132, 1, 115],\n",
       " [21, 94, 4, 132, 1, 115, 4],\n",
       " [21, 94, 4, 132, 1, 115, 4, 132],\n",
       " [21, 94, 4, 132, 1, 115, 4, 132, 236],\n",
       " [6, 942],\n",
       " [6, 942, 676],\n",
       " [6, 942, 676, 2],\n",
       " [6, 942, 676, 2, 3],\n",
       " [6, 942, 676, 2, 3, 16],\n",
       " [6, 942, 676, 2, 3, 16, 52],\n",
       " [445, 12],\n",
       " [445, 12, 150],\n",
       " [445, 12, 150, 1],\n",
       " [445, 12, 150, 1, 1564],\n",
       " [445, 12, 150, 1, 1564, 4],\n",
       " [445, 12, 150, 1, 1564, 4, 943],\n",
       " [39, 56],\n",
       " [39, 56, 40],\n",
       " [39, 56, 40, 3],\n",
       " [39, 56, 40, 3, 16],\n",
       " [39, 56, 40, 3, 16, 132],\n",
       " [39, 56, 40, 3, 16, 132, 74],\n",
       " [39, 56, 40, 3, 16, 132, 74, 47],\n",
       " [39, 56, 40, 3, 16, 132, 74, 47, 369],\n",
       " [9, 539],\n",
       " [9, 539, 26],\n",
       " [9, 539, 26, 163],\n",
       " [9, 539, 26, 163, 1565],\n",
       " [9, 539, 26, 163, 1565, 80],\n",
       " [9, 539, 26, 163, 1565, 80, 97],\n",
       " [44, 26],\n",
       " [44, 26, 1566],\n",
       " [44, 26, 1566, 888],\n",
       " [44, 26, 1566, 888, 9],\n",
       " [44, 26, 1566, 888, 9, 944],\n",
       " [44, 26, 1566, 888, 9, 944, 19],\n",
       " [44, 26, 1566, 888, 9, 944, 19, 155],\n",
       " [14, 120],\n",
       " [14, 120, 1567],\n",
       " [14, 120, 1567, 1568],\n",
       " [105, 5],\n",
       " [105, 5, 700],\n",
       " [105, 5, 700, 9],\n",
       " [298, 945],\n",
       " [119, 1],\n",
       " [119, 1, 308],\n",
       " [119, 1, 308, 55],\n",
       " [119, 1, 308, 55, 1569],\n",
       " [119, 1, 308, 55, 1569, 30],\n",
       " [119, 1, 308, 55, 1569, 30, 1],\n",
       " [119, 1, 308, 55, 1569, 30, 1, 446],\n",
       " [119, 1, 308, 55, 1569, 30, 1, 446, 156],\n",
       " [2, 123],\n",
       " [2, 123, 35],\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create input sequences\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56de0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  885],\n",
       "       [   0,    0,    0, ...,    1,  885,    4],\n",
       "       [   0,    0,    0, ...,  885,    4,   41],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 3552,    1,  885],\n",
       "       [   0,    0,    0, ...,    1,  885,    4],\n",
       "       [   0,    0,    0, ...,  885,    4,   41]],\n",
       "      shape=(15245, 14), dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##apply pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03880a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 16:52:11.667267: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-12-10 16:52:11.667475: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:160] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-12-10 16:52:11.667483: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-12-10 16:52:11.667487: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-12-10 16:52:11.667490: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: CHENWJYHD6Q3\n",
      "2025-12-10 16:52:11.667493: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: CHENWJYHD6Q3\n",
      "2025-12-10 16:52:11.667616: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 580.95.5\n",
      "2025-12-10 16:52:11.667636: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 580.95.5\n",
      "2025-12-10 16:52:11.667638: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 580.95.5\n"
     ]
    }
   ],
   "source": [
    "## Split into train and test sets\n",
    "# Force CPU to avoid GPU JIT issues\n",
    "# Add this as the first cell in the notebook, before any TensorFlow imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')  # Explicitly disable GPU\n",
    "X,y = input_sequences[:,:-1], input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a99c053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1,  885],\n",
       "       [   0,    0,    0, ...,    1,  885,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0, 3552,    1],\n",
       "       [   0,    0,    0, ..., 3552,    1,  885],\n",
       "       [   0,    0,    0, ...,    1,  885,    4]],\n",
       "      shape=(15245, 13), dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8603025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([885,   4,  41, ..., 885,   4,  41], shape=(15245,), dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "740fb2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(15245, 3553))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc854e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a78c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">355,300</span> \n",
       "\n",
       " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">100,400</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3553</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">358,853</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " embedding (\u001b[38;5;33mEmbedding\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m100\u001b[0m)               \u001b[38;5;34m355,300\u001b[0m \n",
       "\n",
       " lstm (\u001b[38;5;33mLSTM\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m150\u001b[0m)               \u001b[38;5;34m150,600\u001b[0m \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m150\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   \u001b[38;5;34m100,400\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3553\u001b[0m)                  \u001b[38;5;34m358,853\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">965,153</span> (3.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m965,153\u001b[0m (3.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">965,153</span> (3.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m965,153\u001b[0m (3.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train LSTM \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Embedding(total_words, 100))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f34793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2141 - loss: 3.8657 - val_accuracy: 0.0469 - val_loss: 9.4006\n",
      "Epoch 2/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2268 - loss: 3.7931 - val_accuracy: 0.0485 - val_loss: 9.5475\n",
      "Epoch 3/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2396 - loss: 3.7188 - val_accuracy: 0.0495 - val_loss: 9.6334\n",
      "Epoch 4/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2514 - loss: 3.6567 - val_accuracy: 0.0489 - val_loss: 9.7326\n",
      "Epoch 5/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2613 - loss: 3.5803 - val_accuracy: 0.0449 - val_loss: 9.8426\n",
      "Epoch 6/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2698 - loss: 3.5264 - val_accuracy: 0.0469 - val_loss: 9.9407\n",
      "Epoch 7/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.2857 - loss: 3.4570 - val_accuracy: 0.0449 - val_loss: 10.0170\n",
      "Epoch 8/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.2962 - loss: 3.3981 - val_accuracy: 0.0430 - val_loss: 10.1336\n",
      "Epoch 9/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3091 - loss: 3.3379 - val_accuracy: 0.0459 - val_loss: 10.2690\n",
      "Epoch 10/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3209 - loss: 3.2778 - val_accuracy: 0.0423 - val_loss: 10.3567\n",
      "Epoch 11/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3315 - loss: 3.2290 - val_accuracy: 0.0436 - val_loss: 10.4178\n",
      "Epoch 12/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3381 - loss: 3.1704 - val_accuracy: 0.0499 - val_loss: 10.5734\n",
      "Epoch 13/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3463 - loss: 3.1202 - val_accuracy: 0.0476 - val_loss: 10.5892\n",
      "Epoch 14/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3554 - loss: 3.0742 - val_accuracy: 0.0469 - val_loss: 10.7306\n",
      "Epoch 15/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3681 - loss: 3.0230 - val_accuracy: 0.0449 - val_loss: 10.8251\n",
      "Epoch 16/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3752 - loss: 2.9742 - val_accuracy: 0.0430 - val_loss: 10.8746\n",
      "Epoch 17/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3860 - loss: 2.9296 - val_accuracy: 0.0439 - val_loss: 10.9389\n",
      "Epoch 18/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3957 - loss: 2.8829 - val_accuracy: 0.0443 - val_loss: 11.0477\n",
      "Epoch 19/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.3986 - loss: 2.8393 - val_accuracy: 0.0426 - val_loss: 11.0811\n",
      "Epoch 20/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4045 - loss: 2.8026 - val_accuracy: 0.0453 - val_loss: 11.2043\n",
      "Epoch 21/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4160 - loss: 2.7602 - val_accuracy: 0.0433 - val_loss: 11.2507\n",
      "Epoch 22/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4219 - loss: 2.7198 - val_accuracy: 0.0443 - val_loss: 11.3216\n",
      "Epoch 23/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4325 - loss: 2.6800 - val_accuracy: 0.0453 - val_loss: 11.4394\n",
      "Epoch 24/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4371 - loss: 2.6354 - val_accuracy: 0.0449 - val_loss: 11.4948\n",
      "Epoch 25/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4419 - loss: 2.6045 - val_accuracy: 0.0420 - val_loss: 11.5433\n",
      "Epoch 26/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4538 - loss: 2.5627 - val_accuracy: 0.0407 - val_loss: 11.6202\n",
      "Epoch 27/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4598 - loss: 2.5168 - val_accuracy: 0.0443 - val_loss: 11.7201\n",
      "Epoch 28/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4675 - loss: 2.4871 - val_accuracy: 0.0446 - val_loss: 11.7205\n",
      "Epoch 29/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4761 - loss: 2.4518 - val_accuracy: 0.0459 - val_loss: 11.8664\n",
      "Epoch 30/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4802 - loss: 2.4185 - val_accuracy: 0.0469 - val_loss: 11.9215\n",
      "Epoch 31/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4897 - loss: 2.3853 - val_accuracy: 0.0453 - val_loss: 11.9743\n",
      "Epoch 32/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4930 - loss: 2.3590 - val_accuracy: 0.0453 - val_loss: 12.0276\n",
      "Epoch 33/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.4962 - loss: 2.3286 - val_accuracy: 0.0443 - val_loss: 12.1138\n",
      "Epoch 34/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5087 - loss: 2.2890 - val_accuracy: 0.0439 - val_loss: 12.1603\n",
      "Epoch 35/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5093 - loss: 2.2589 - val_accuracy: 0.0430 - val_loss: 12.2205\n",
      "Epoch 36/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5156 - loss: 2.2277 - val_accuracy: 0.0430 - val_loss: 12.3279\n",
      "Epoch 37/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5236 - loss: 2.1979 - val_accuracy: 0.0443 - val_loss: 12.3606\n",
      "Epoch 38/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5327 - loss: 2.1654 - val_accuracy: 0.0443 - val_loss: 12.3956\n",
      "Epoch 39/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5268 - loss: 2.1494 - val_accuracy: 0.0459 - val_loss: 12.4738\n",
      "Epoch 40/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5376 - loss: 2.1153 - val_accuracy: 0.0462 - val_loss: 12.5571\n",
      "Epoch 41/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5417 - loss: 2.0919 - val_accuracy: 0.0443 - val_loss: 12.5720\n",
      "Epoch 42/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5482 - loss: 2.0597 - val_accuracy: 0.0482 - val_loss: 12.6288\n",
      "Epoch 43/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5533 - loss: 2.0305 - val_accuracy: 0.0472 - val_loss: 12.7165\n",
      "Epoch 44/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5616 - loss: 2.0038 - val_accuracy: 0.0476 - val_loss: 12.7644\n",
      "Epoch 45/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5717 - loss: 1.9766 - val_accuracy: 0.0443 - val_loss: 12.8352\n",
      "Epoch 46/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5742 - loss: 1.9430 - val_accuracy: 0.0462 - val_loss: 12.8543\n",
      "Epoch 47/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5758 - loss: 1.9296 - val_accuracy: 0.0462 - val_loss: 12.9353\n",
      "Epoch 48/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5835 - loss: 1.9015 - val_accuracy: 0.0472 - val_loss: 12.9844\n",
      "Epoch 49/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5847 - loss: 1.8782 - val_accuracy: 0.0485 - val_loss: 13.0462\n",
      "Epoch 50/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5876 - loss: 1.8538 - val_accuracy: 0.0485 - val_loss: 13.0916\n",
      "Epoch 51/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5970 - loss: 1.8346 - val_accuracy: 0.0476 - val_loss: 13.1500\n",
      "Epoch 52/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6018 - loss: 1.8106 - val_accuracy: 0.0439 - val_loss: 13.1772\n",
      "Epoch 53/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6069 - loss: 1.7820 - val_accuracy: 0.0449 - val_loss: 13.2874\n",
      "Epoch 54/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6091 - loss: 1.7621 - val_accuracy: 0.0417 - val_loss: 13.3310\n",
      "Epoch 55/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6118 - loss: 1.7453 - val_accuracy: 0.0466 - val_loss: 13.3816\n",
      "Epoch 56/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6195 - loss: 1.7199 - val_accuracy: 0.0459 - val_loss: 13.4150\n",
      "Epoch 57/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6237 - loss: 1.6949 - val_accuracy: 0.0417 - val_loss: 13.4437\n",
      "Epoch 58/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6299 - loss: 1.6743 - val_accuracy: 0.0453 - val_loss: 13.4882\n",
      "Epoch 59/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6291 - loss: 1.6554 - val_accuracy: 0.0423 - val_loss: 13.5258\n",
      "Epoch 60/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6338 - loss: 1.6380 - val_accuracy: 0.0443 - val_loss: 13.6062\n",
      "Epoch 61/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6413 - loss: 1.6171 - val_accuracy: 0.0443 - val_loss: 13.6372\n",
      "Epoch 62/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6455 - loss: 1.5975 - val_accuracy: 0.0469 - val_loss: 13.6949\n",
      "Epoch 63/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6474 - loss: 1.5829 - val_accuracy: 0.0426 - val_loss: 13.7101\n",
      "Epoch 64/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6510 - loss: 1.5687 - val_accuracy: 0.0410 - val_loss: 13.8203\n",
      "Epoch 65/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6565 - loss: 1.5353 - val_accuracy: 0.0426 - val_loss: 13.8714\n",
      "Epoch 66/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6621 - loss: 1.5209 - val_accuracy: 0.0446 - val_loss: 13.8901\n",
      "Epoch 67/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6630 - loss: 1.5112 - val_accuracy: 0.0446 - val_loss: 13.9436\n",
      "Epoch 68/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6668 - loss: 1.4822 - val_accuracy: 0.0417 - val_loss: 13.9769\n",
      "Epoch 69/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6718 - loss: 1.4637 - val_accuracy: 0.0443 - val_loss: 14.0392\n",
      "Epoch 70/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6790 - loss: 1.4482 - val_accuracy: 0.0433 - val_loss: 14.0798\n",
      "Epoch 71/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6775 - loss: 1.4329 - val_accuracy: 0.0453 - val_loss: 14.1167\n",
      "Epoch 72/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6819 - loss: 1.4207 - val_accuracy: 0.0426 - val_loss: 14.1395\n",
      "Epoch 73/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6780 - loss: 1.4115 - val_accuracy: 0.0413 - val_loss: 14.1813\n",
      "Epoch 74/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6874 - loss: 1.3887 - val_accuracy: 0.0439 - val_loss: 14.2569\n",
      "Epoch 75/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6893 - loss: 1.3704 - val_accuracy: 0.0413 - val_loss: 14.3153\n",
      "Epoch 76/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6933 - loss: 1.3631 - val_accuracy: 0.0449 - val_loss: 14.3725\n",
      "Epoch 77/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6927 - loss: 1.3475 - val_accuracy: 0.0462 - val_loss: 14.3767\n",
      "Epoch 78/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.6942 - loss: 1.3405 - val_accuracy: 0.0466 - val_loss: 14.4195\n",
      "Epoch 79/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7009 - loss: 1.3234 - val_accuracy: 0.0413 - val_loss: 14.4318\n",
      "Epoch 80/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7097 - loss: 1.2921 - val_accuracy: 0.0430 - val_loss: 14.4933\n",
      "Epoch 81/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7060 - loss: 1.2891 - val_accuracy: 0.0469 - val_loss: 14.5389\n",
      "Epoch 82/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7078 - loss: 1.2854 - val_accuracy: 0.0439 - val_loss: 14.5261\n",
      "Epoch 83/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7091 - loss: 1.2631 - val_accuracy: 0.0439 - val_loss: 14.6062\n",
      "Epoch 84/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.7200 - loss: 1.2478 - val_accuracy: 0.0449 - val_loss: 14.6878\n",
      "Epoch 85/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7223 - loss: 1.2330 - val_accuracy: 0.0433 - val_loss: 14.7078\n",
      "Epoch 86/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7237 - loss: 1.2167 - val_accuracy: 0.0443 - val_loss: 14.7315\n",
      "Epoch 87/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7228 - loss: 1.2149 - val_accuracy: 0.0456 - val_loss: 14.7631\n",
      "Epoch 88/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7237 - loss: 1.1966 - val_accuracy: 0.0410 - val_loss: 14.8449\n",
      "Epoch 89/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7285 - loss: 1.1843 - val_accuracy: 0.0397 - val_loss: 14.8948\n",
      "Epoch 90/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7295 - loss: 1.1821 - val_accuracy: 0.0394 - val_loss: 14.9094\n",
      "Epoch 91/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7321 - loss: 1.1671 - val_accuracy: 0.0410 - val_loss: 14.9480\n",
      "Epoch 92/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7375 - loss: 1.1524 - val_accuracy: 0.0377 - val_loss: 14.9533\n",
      "Epoch 93/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7378 - loss: 1.1373 - val_accuracy: 0.0417 - val_loss: 15.0051\n",
      "Epoch 94/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7420 - loss: 1.1254 - val_accuracy: 0.0374 - val_loss: 15.1104\n",
      "Epoch 95/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7446 - loss: 1.1159 - val_accuracy: 0.0400 - val_loss: 15.1027\n",
      "Epoch 96/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7448 - loss: 1.1132 - val_accuracy: 0.0387 - val_loss: 15.1347\n",
      "Epoch 97/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7448 - loss: 1.1061 - val_accuracy: 0.0417 - val_loss: 15.1719\n",
      "Epoch 98/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7491 - loss: 1.0863 - val_accuracy: 0.0384 - val_loss: 15.1452\n",
      "Epoch 99/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7530 - loss: 1.0740 - val_accuracy: 0.0439 - val_loss: 15.2051\n",
      "Epoch 100/100\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7520 - loss: 1.0794 - val_accuracy: 0.0423 - val_loss: 15.2104\n"
     ]
    }
   ],
   "source": [
    "#Train the LSTM model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82624fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa98288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: 'to be or not to' -> Predicted Next Word: 'him'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"to be or not to\"\n",
    "max_sequence_len = model.input_shape[1] + 1\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Input Text: '{input_text}' -> Predicted Next Word: '{next_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f62959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "model.save('lstm_macbeth_model.h5')\n",
    "\n",
    "#save tokenizer\n",
    "import pickle\n",
    "with open('tokenizer_macbeth.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25763205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: 'that doe cling' -> Predicted Next Word: 'or'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"that doe cling\"\n",
    "max_sequence_len = model.input_shape[1] + 1\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Input Text: '{input_text}' -> Predicted Next Word: '{next_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27fc3a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">355,300</span> \n",
       "\n",
       " gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">113,400</span> \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">75,600</span> \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3553</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">358,853</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m100\u001b[0m)               \u001b[38;5;34m355,300\u001b[0m \n",
       "\n",
       " gru (\u001b[38;5;33mGRU\u001b[0m)                        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m150\u001b[0m)               \u001b[38;5;34m113,400\u001b[0m \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m150\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " gru_1 (\u001b[38;5;33mGRU\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                    \u001b[38;5;34m75,600\u001b[0m \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3553\u001b[0m)                  \u001b[38;5;34m358,853\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">903,153</span> (3.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m903,153\u001b[0m (3.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">903,153</span> (3.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m903,153\u001b[0m (3.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GRU Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# Build the model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Input(shape=(X_train.shape[1],)))\n",
    "model_gru.add(Embedding(total_words, 100))\n",
    "model_gru.add(GRU(150, return_sequences=True))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(GRU(100))\n",
    "model_gru.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model_gru.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8f700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfs_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
